#undef NDEBUG
#include <stdlib.h>
#include <stdio.h>
#include <malloc.h>
#include <assert.h>
#include <xmmintrin.h>
#include <emmintrin.h>
#include <errno.h>
#include <unistd.h>
#include "butterfly.h"
#include "blas.h"
#include "fastsht_error.h"
#include "butterfly_utils.h"

/* Common page size on AMD64 */
#define BUF_ALIGN 4096

typedef __m128d m128d;
typedef __m128i m128i;
typedef __m128 m128;
typedef union {
  char c[16];
  m128i v;
} v16c;


{{py:
nvecs_specs = [2, None]

nvecarg_specs = [', size_t nvecs' if x is None else '' for x in nvecs_specs]
nvecs_instances = zip(
    nvecs_specs, # nvecs
    ['_%d' % x if x is not None else '' for x in nvecs_specs], # suffix
    [', size_t nvecs' if x is None else '' for x in nvecs_specs],     # trailing_args
    ['#define nvecs %d' % x if x is not None else '' for x in nvecs_specs], # define
    ['#undef nvecs' if x is not None else '' for x in nvecs_specs]) # undef

}}


/*
Copies 'a' to the locations marked with 0 in the mask,
and 'b' to the locations marked with 1.

The target buffer is the concatenation of target1 and target2.
The lengths of each of the buffer parts is in units of nvecs.
*/
{{py:
def scatter_name(group, should_add, nvecs):
    return 'bfm_scatter_group%d_%s%s' % (
        group,
        'add' if should_add else 'noadd',
        '_%d' % nvecs if nvecs else ''
        )
}}

{{for group in [0, 1]}}
{{for nvecs, suffix, trailing_args, define, undefine in nvecs_instances}}
{{for should_add in [False, True]}}
const char *{{scatter_name(group, should_add, nvecs)}}(
    const char *restrict mask, 
    double *restrict target1,
    double *restrict target2,
    const double *restrict source,
    size_t len1, size_t len2{{trailing_args}}) {
  /* Time spent is this routine is likely due to cache effects...be
     sure to profile on in-cache data.

     This implementation appears to be rather fast; tried playing with
     eliminating branches through bit operations etc. and it did not give
     speedups.  Best way forward to speed this up is likely to pack
     more information into each char in the mask, and use a switch to
     dispatch to multiple load/stores. */
  {{define}}
  int j;
  const char *restrict end;
  int m;
  int x = 0;
  m128d val;
  {{if should_add}}m128d tmp;{{endif}}
  assert(nvecs % 2 == 0);
  assert((size_t)target1 % 16 == 0);
  assert((size_t)target2 % 16 == 0);
  assert((size_t)source % 16 == 0);
  {{for idx in [1, 2]}}
  end = mask + len{{idx}};
  while (mask != end) {
    m = *mask++;
    if (m == {{group}}) {
      for (j = 0; j != nvecs / 2; ++j) {
        val = _mm_load_pd(source);
        source += 2;
        {{if should_add}}
        tmp = _mm_load_pd(target{{idx}});
        val = _mm_add_pd(val, tmp);
        {{endif}}
        _mm_store_pd(target{{idx}}, val);
        target{{idx}} += 2;
      }
    } else {
      target{{idx}} += nvecs;
    }
    x++;
  }
  {{endfor}}
  return mask;
  {{undefine}}
}
{{endfor}}
{{endfor}}
{{endfor}}

/* Runtime dispatcher for scatter */
const char *bfm_scatter(
    const char *restrict mask, 
    double *restrict target1,
    double *restrict target2,
    const double *restrict source,
    size_t len1, size_t len2, size_t nvecs,
    int group, int should_add) {
  {{py: args='mask, target1, target2, source, len1, len2'}}
  {{for group in [0, 1]}}
  {{for should_add in [False, True]}}
  if ((group == {{group}}) && ({{'' if should_add else '!'}}should_add))
 {
   if (0) {
    {{for nvecs in [x for x in nvecs_specs if x is not None]}}
    if (nvecs == {{nvecs}}) {
      return {{scatter_name(group, should_add, nvecs)}}({{args}});
    }
    {{endfor}}
   }    else {
      return {{scatter_name(group, should_add, None)}}({{args}}, nvecs);
    }
  }
  {{endfor}}
  {{endfor}}
  check(0, "bfm_scatter: Invalid group or should_add argument");
}


/*
Utils
*/

static void print_array(char *msg, double* arr, bfm_index_t len) {
  bfm_index_t i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}

bfm_plan *bfm_create_plan(size_t k_max, size_t nblocks_max, size_t nvecs,
                          sem_t *mem_semaphore, sem_t *cpu_semaphore) {
  bfm_plan *plan;
  size_t i;
  if (k_max == 0) k_max++;
  plan = malloc(sizeof(bfm_plan));
  plan->k_max = k_max;
  plan->nblocks_max = nblocks_max;
  plan->nvecs = nvecs;
  plan->mem_semaphore = mem_semaphore;
  plan->cpu_semaphore = cpu_semaphore;
  plan->chunks_allocated = plan->chunk_stack_size = nblocks_max + 2;
  plan->vector_chunk_stack = malloc(sizeof(void*[plan->chunk_stack_size]));
  for (i = 0; i != plan->chunks_allocated; ++i) {
    plan->vector_chunk_stack[i] = memalign(BUF_ALIGN, sizeof(double[k_max * nvecs]));
  }
  plan->y_buf = memalign(BUF_ALIGN, sizeof(double[2 * k_max * nvecs]));
  return plan;
}

void bfm_destroy_plan(bfm_plan *plan) {
  int i;
  if (!plan) return;
  assert(plan->chunk_stack_size == plan->chunks_allocated);
  for (i = 0; i != plan->chunk_stack_size; ++i) {
    free(plan->vector_chunk_stack[i]);
  }
  free((double *)plan->vector_chunk_stack);
  free(plan->y_buf);
  free(plan);
}


struct _bfm_transpose_apply_context {
  bfm_plan *plan;
  pull_func_t pull_func;
  void *caller_ctx;
  char *matrix_data;
  char **node_heap;
  char **residual_payload_headers;
  double *target;
  int current_root_idx;
  int add_push;
};

static inline double *acquire_vector_chunk(bfm_plan *plan) {
  assert(plan->chunk_stack_size > 0);
  size_t idx = --plan->chunk_stack_size;
  double *r = plan->vector_chunk_stack[idx];
  plan->vector_chunk_stack[idx] = NULL;
  return r;
}

static inline void release_vector_chunk(bfm_plan *plan, double *chunk) {
  assert(plan->chunk_stack_size < plan->chunks_allocated);
  plan->vector_chunk_stack[plan->chunk_stack_size++] = chunk;
}

static void read_interpolation_block(char **head, char **mask, double **interpolant,
                                     size_t n, size_t k) {
  *mask = *head;
  *head += sizeof(char[n]);
  *head = skip_padding(*head);
  *interpolant = (double*)*head;
  *head += sizeof(double[(n - k) * k]);
}

static void transpose_apply_interpolation_block(
                         char **head, double *output_left, double *output_right,
                         double *input, double *y_buf,
                         size_t n_left, size_t n_right, size_t k,
                         size_t nvecs, int should_add) {
  char *mask;
  double *interpolant;
  size_t n = n_left + n_right;
  read_interpolation_block(head, &mask, &interpolant, n, k);

  bfm_scatter(mask, output_left, output_right, input, n_left, n_right, nvecs, 0, should_add);
  dgemm_ccc(input, interpolant, y_buf, nvecs, n - k, k, 0.0);
  bfm_scatter(mask, output_left, output_right, y_buf, n_left, n_right, nvecs, 1, should_add);
}

static void copy_vectors(double *input, double *target, size_t target_start, size_t target_stop,
                         size_t nvecs, int should_add) {
  size_t i;
  size_t n = (target_stop - target_start) * nvecs;
  target += target_start * nvecs;
  if (should_add) {
    for (i = 0; i != n; ++i) {
      target[i] += input[i];
    }
  } else {
    for (i = 0; i != n; ++i) {
      target[i] = input[i];
    }
  }
}

#if 1

static void enter_mem_section(bfm_plan *plan) {}
static void exit_mem_section(bfm_plan *plan) {}

#else

static void enter_mem_section(bfm_plan *plan) {
  int r;
  while (1) {
    while ((r = sem_trywait(plan->mem_semaphore)) == EINTR) {};
    if (r == 0) {
      /* Got memory */
      return;
    } else {
    /* No memory lock available right away, yield the CPU and
       reacquire it in order to give CPU-bound threads a chance to take
       over. */
    sem_post(plan->cpu_semaphore);
    usleep(100);
    while ((r = sem_wait(plan->cpu_semaphore)) == EINTR) {};
    }
  }
}

static void exit_mem_section(bfm_plan *plan) {
  sem_post(plan->mem_semaphore);
}

#endif

static size_t transpose_apply_node(bfm_transpose_apply_context *ctx,
                                   size_t inode,
                                   size_t target_start,
                                   double **input_blocks) {
  char *node_data = ctx->node_heap[inode];
  size_t nblocks = read_index(&node_data);

  int is_root = (input_blocks == NULL);
  bfm_plan *plan = ctx->plan;
  double *input_block;

  char *payloads[((nblocks == 0) ? 1 : nblocks) + 1];

  if (is_root) {
    char *payload_head = ctx->residual_payload_headers[ctx->current_root_idx];
    size_t n = read_int64(&payload_head);
    assert((n == nblocks) || (nblocks == 0 && n == 1));
    read_pointer_list(&payload_head, payloads, n + 1, ctx->matrix_data);
  }

  if (nblocks == 0) {
    /* Leaf node, simply push buffer to output vectors. */
    size_t n = read_index(&node_data);
    size_t target_stop = target_start + n;
    if (is_root) {
      /* Simply stream data straight through */
      assert(!ctx->add_push); /* TODO Do something sane */
      ctx->pull_func(ctx->target, target_start, target_stop, plan->nvecs, payloads[0],
                     payloads[1] - payloads[0], ctx->caller_ctx);
    } else {
      /* TODO: Avoid this copy at leafs, go straight to target in the nblocks=2 case */
      input_block = input_blocks[0];
      copy_vectors(input_block, ctx->target, target_start, target_stop, plan->nvecs,
                   ctx->add_push);
      release_vector_chunk(plan, input_block);
    }
    return target_stop;
  } else {
    bfm_index_t *block_heights = (bfm_index_t*)node_data;
    node_data += sizeof(bfm_index_t[nblocks]);
    char *left_child_data = ctx->node_heap[2 * inode];
    char *right_child_data = ctx->node_heap[2 * inode + 1];
    size_t nleft = read_index(&left_child_data);
    size_t nright = read_index(&right_child_data);
    assert((nleft == 0 && nright == 0) || 
           (nleft == nblocks / 2 && nright == nblocks / 2));
    bfm_index_t *left_child_block_heights = (bfm_index_t*)left_child_data;
    bfm_index_t *right_child_block_heights = (bfm_index_t*)right_child_data;

    /* Process interpolation nodes. */
    size_t input_pos = 0; /* only used if is_root */
    double *out_left_list[nblocks / 2], *out_right_list[nblocks / 2];
    for (size_t i = 0; i != nblocks / 2; ++i) {
      double *out_left = out_left_list[i] = acquire_vector_chunk(plan);
      double *out_right = out_right_list[i] = acquire_vector_chunk(plan);
      size_t n_left = left_child_block_heights[i];
      size_t n_right = right_child_block_heights[i];
      assert(n_left <= plan->k_max && n_right <= plan->k_max);
      
      /* Loop over cases T and B */
      for (int j = 0; j != 2; ++j) {
        size_t k = block_heights[2 * i + j];
        assert(k <= plan->k_max);
        if (is_root) {
          input_block = acquire_vector_chunk(plan);
          ctx->pull_func(input_block, input_pos, input_pos + k,
                         plan->nvecs, payloads[2 * i + j],
                         payloads[2 * i + j + 1] - payloads[2 * i + j],
                         ctx->caller_ctx);
          input_pos += k;
        } else {
          input_block = input_blocks[2 * i + j];
          input_blocks[2 * i + j] = NULL;
        }
        int should_add = j;
        transpose_apply_interpolation_block(&node_data, out_left, out_right,
                                            input_block, plan->y_buf,
                                            n_left, n_right, k, plan->nvecs,
                                            should_add);
        release_vector_chunk(plan, input_block);
      }
    }
    
    /* Recurse */
    size_t idx = target_start;
    idx = transpose_apply_node(ctx, 2 * inode, idx, out_left_list);
    idx = transpose_apply_node(ctx, 2 * inode + 1, idx, out_right_list);
    return idx;
  }
}

int bfm_transpose_apply_d(bfm_plan *plan,
                          char *matrix_data,
                          pull_func_t pull_func,
                          double *target,
                          size_t target_len,
                          void *caller_ctx) {
  bfm_transpose_apply_context ctx;
  bfm_matrix_data_info info;
  char *head = matrix_data;
  ctx.caller_ctx = caller_ctx;
  ctx.pull_func = pull_func;
  ctx.matrix_data = matrix_data;
  ctx.target = target;
  ctx.plan = plan;
  check((size_t)matrix_data % 16 == 0, "matrix_data not 128-bit aligned");
  
  /* Read in tree. Shape of the S-matrix forest is determined by the
     start level and stop level. Offsets to each node is stored in a
     heap structure that is present; we translate it to pointers in
     memory.

     The heap base pointer is the virtual zero level, i.e., it
     incorporates the subtraction of the first index.
  */
  head = bfm_query_matrix_data(head, &info);

  check(target_len == plan->nvecs * info.ncols, "target_len does not match ncols * nvecs");


  /* Read residual matrix payload data */
  char *residual_payload_headers[info.first_level_size];
  ctx.residual_payload_headers = residual_payload_headers;
  read_pointer_list(&head, residual_payload_headers, info.first_level_size, matrix_data);

  /* Read tree node pointers in binary heap structure */
  char *heap_buf[info.heap_size];
  ctx.node_heap = heap_buf - info.heap_first_index;

  read_pointer_list(&head, heap_buf, info.heap_size, matrix_data);


  /* Start to apply the root level. 

     We signal whether we're in the first iteration using the add_push
     flag, which the caller can inspect and (optionally) zero the output buffer.
  */
  ctx.add_push = 0;
  ctx.current_root_idx = 0;
  size_t start = 0;
  for (size_t inode = info.heap_first_index;
       inode != info.heap_first_index + info.first_level_size;
       ++inode) {
    start = transpose_apply_node(&ctx, inode, start, NULL);
    ctx.current_root_idx++;
  }
  assert(start == info.ncols);
  return 0;
}

char *bfm_query_matrix_data(char *head, bfm_matrix_data_info *info) {
  info->nrows = read_int32(&head);
  info->ncols = read_int32(&head);
  info->k_max = read_int32(&head);
  info->nblocks_max = read_int32(&head);
  info->element_count = read_int64(&head);
  info->first_level_size = read_int32(&head);
  info->heap_size = read_int32(&head);
  info->heap_first_index = read_int32(&head);
  read_int32(&head);
  return head;
}
