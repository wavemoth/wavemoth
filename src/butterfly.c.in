#undef NDEBUG
#include <stdlib.h>
#include <stdio.h>
#include <malloc.h>
#include <assert.h>
#include <xmmintrin.h>
#include <emmintrin.h>
#include "butterfly.h"
#include "blas.h"
#include "fastsht_error.h"

/* Common page size on AMD64 */
#define BUF_ALIGN 4096

typedef __m128d m128d;
typedef __m128i m128i;
typedef __m128 m128;
typedef union {
  char c[16];
  m128i v;
} v16c;

/*
Spread and merge
*/

{{py:
nvecs_specs = [2, None]

nvecarg_specs = [', int32_t nvecs' if x is None else '' for x in nvecs_specs]
nvecs_instances = zip(
    nvecs_specs, # nvecs
    ['_%d' % x if x is not None else '' for x in nvecs_specs], # suffix
    [', int32_t nvecs' if x is None else '' for x in nvecs_specs],     # trailing_args
    ['#define nvecs %d' % x if x is not None else '' for x in nvecs_specs], # define
    ['#undef nvecs' if x is not None else '' for x in nvecs_specs]) # undef

}}


{{for nvecs, suffix, trailing_args, define, undefine in nvecs_instances}}
static char *merge_vectors{{suffix}}(char *mask, double *x, double *a, double *b,
                           int32_t alen, int32_t blen{{trailing_args}}) {
  {{define}}
  int j;
  char *end = mask + alen + blen;
  int m;
  m128d val;
  assert(nvecs % 2 == 0);
  assert((size_t)x % 16 == 0);
  assert((size_t)a % 16 == 0);
  assert((size_t)b % 16 == 0);
  while (mask != end) {
    m = 2 * *mask++;
    for (j = 0; j != nvecs / 2; ++j) {
      val = _mm_load_pd(x);
      x += 2;
      _mm_store_pd(a, val);
      _mm_store_pd(b, val);
      a += (2 - m);
      b += m;
    }
  }
  return mask;
  {{undefine}}
}
{{endfor}}



/*
Copies 'a' to the locations marked with 0 in the mask,
and 'b' to the locations marked with 1.

The target buffer is the concatenation of target1 and target2.
The lengths of each of the buffer parts is in units of nvecs.
*/
{{for should_add in [False, True]}}
{{for should_not in [False, True]}}
{{for nvecs, suffix, trailing_args, define, undefine in nvecs_instances}}
const char *bfm_scatter{{'_complement' * should_not}}{{'_add' * should_add}}{{suffix}}(
    const char *restrict mask, 
    double *restrict target1,
    double *restrict target2,
    const double *restrict source,
    int len1, int len2{{trailing_args}}) {
  /* Time spent is this routine is likely due to cache effects...be
     sure to profile on in-cache data.

     This implementation appears to be rather fast; tried playing with
     eliminating branches through bit operations etc. and it did not give
     speedups.  Best way forward to speed this up is likely to pack
     more information into each char in the mask, and use a switch to
     dispatch to multiple load/stores. */
  {{define}}
  int j;
  const char *restrict end;
  int m;
  int x = 0;
  m128d val;
  {{if should_add}}m128d tmp;{{endif}}
  assert(nvecs % 2 == 0);
  assert((size_t)target1 % 16 == 0);
  assert((size_t)target2 % 16 == 0);
  assert((size_t)source % 16 == 0);
  {{for idx in [1, 2]}}
  end = mask + len{{idx}};
  while (mask != end) {
    m = *mask++;
    if ({{'!' * should_not}}m) {
      for (j = 0; j != nvecs / 2; ++j) {
        val = _mm_load_pd(source);
        source += 2;
        {{if should_add}}
        tmp = _mm_load_pd(target{{idx}});
        val = _mm_add_pd(val, tmp);
        {{endif}}
        _mm_store_pd(target{{idx}}, val);
        target{{idx}} += 2;
      }
    } else {
      target{{idx}} += nvecs;
    }
    x++;
  }
  {{endfor}}
  return mask;
  {{undefine}}
}
{{endfor}}
{{endfor}}
{{endfor}}

/*
Utils
*/

static void print_array(char *msg, double* arr, bfm_index_t len) {
  bfm_index_t i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}


static INLINE char *skip_padding(char *ptr) {
  size_t m = (size_t)ptr % 16;
  if (m == 0) {
    return ptr;
  } else { 
    return ptr + 16 - m;
  }
}

/*
Type implementations
*/


/*static int dense_rowmajor_right_d(char *matrixdata, double *x, double *y,
                                  bfm_index_t nrow, bfm_index_t ncol, bfm_index_t nvec) {
  double *matrix = (double*)(matrixdata + 16);
  /* dgemm uses Fortran-order, so do transposed multiply;
     C-order: y^T = x^T * matrix^T
     Fortran-order: y = x * matrix
/
  int m = nvec, n = nrow, k = ncol;
  dgemm('N', 'N', m, n, k, 1.0, x, m, matrix, k, 0.0, y, m);
  return 0;
}*/


static void stack_vectors_d(double *a, bfm_index_t alen, double *b, bfm_index_t blen,
                            double *output) {
  bfm_index_t i;
  for (i = 0; i != alen; ++i) {
    output[i] = a[i];
  }
  for (i = 0; i != blen; ++i) {
    output[alen + i] = b[i];
  }
}

/*
  Filter input x into two parts: Those hit with identity matrix which goes
  to y, and those going to a set of temporary vectors tmp_vecs which
  will be multiplied with the interpolation matrix and then added to y.
  
  input is n-by-nvec, output is k-by-nvec
*/
volatile char globchar;
static void access_mem(char *head, size_t size) {
  char r = 0;
  int i;
  for (i = 0; i < size; i += 64) {
    r += head[i];
  }
  globchar = r;
}

static char *apply_interpolation_d(char *head, double *input, double *output,
                                   int32_t k, int32_t n, int32_t nvec) {
  int i;
  char *next_head;
  double tmp_vecs[nvec * (n - k + 1)];
  if (nvec == 2) {
    head = merge_vectors_2(head, input, output, tmp_vecs, k, n - k);
  } else {
    head = merge_vectors(head, input, output, tmp_vecs, k, n - k, nvec);
  }
  head = skip_padding(head);
  access_mem(head, k * (n - k));
  dgemm_rrr((double*)head, tmp_vecs, output, k, nvec, n - k, 1.0);
  head += sizeof(double[k * (n - k)]);
  return head;
}

/* Forward declaration */
static char *apply_butterfly_node_d(char *head, bfm_index_t order, double *input,
                                    double *output, double *buffer,
                                    bfm_index_t nrows, bfm_index_t ncols,
                                    bfm_index_t nvecs);

static INLINE char *recurse_d(char *head, bfm_index_t order,
                              double *input, bfm_index_t nrows, 
                              bfm_index_t ncols,
                              bfm_index_t nvecs,
                              double *output,
                              double *buffer2,
                              double **data_from_first,
                              double **data_from_second,
                              bfm_index_t **out_block_heights,
                              bfm_index_t **out_block_widths_first,
                              bfm_index_t **out_block_widths_second) {
  bfm_index_t *block_widths_first, *block_widths_second;
  printf("%d %d\n", *(bfm_index_t*)head, order);
  assert(*(bfm_index_t*)head == 2 * order);   /* nblocks field */
  head += sizeof(bfm_index_t);
  *out_block_heights = (bfm_index_t*)head; /* [2 * order] */
  bfm_index_t nrows_first = ((bfm_index_t*)head)[2 * order];
  bfm_index_t nrows_second = ((bfm_index_t*)head)[2 * order + 1];
  bfm_index_t col_split = ((bfm_index_t*)head)[2 * order + 2];
  /*checkf(nrows >= nrows_first + nrows_second,
         "nrows=%d, but nrows_first + nrows_second = %d + %d = %d",
         nrows, nrows_first, nrows_second, nrows_first + nrows_second);*/
  head += sizeof(bfm_index_t[2 * order + 3]);
  if (order == 1) {
    /* Parse the two leaf node identity matrices */
    bfm_index_t *ihead = (bfm_index_t*)head;
    assert(ihead[0] == ihead[2] == 0);
    printf("%d %d %d\n", ihead[0], ihead[2], order);
    *out_block_widths_first = ihead + 1;
    *out_block_widths_second = ihead + 3;
    head += sizeof(bfm_index_t[4]);
    nrows_first = (*out_block_widths_first)[0];
    *data_from_first = input;
    *data_from_second = *data_from_first + nrows_first * nvecs;
  } else {
    //    printf("IN\n");
    *data_from_first = output;
    /* Recurse to sub-nodes. */
    *out_block_widths_first = (bfm_index_t*) head;
    //    printf("CONSUMING %d : %lx\n", nrows_first, (size_t)output);
    head = apply_butterfly_node_d(head, order / 2, input, *data_from_first, buffer2, nrows_first,
                                  col_split, nvecs);
    input += col_split * nvecs;
    *data_from_second = *data_from_first + nrows_first * nvecs;
    *out_block_widths_second = (bfm_index_t*) head;
    //printf("CONSUMING %d : %lx\n", nrows_second, (size_t)output);
    head = apply_butterfly_node_d(head, order / 2, input, *data_from_second, buffer2, nrows_second,
                                  ncols - col_split, nvecs);
    //printf("OUT\n");
  }
  return head;
}

static char *apply_butterfly_node_d(char *head, bfm_index_t order, double *input,
                                    double *output, double *buffer,
                                    bfm_index_t nrows, bfm_index_t ncols,
                                    bfm_index_t nvecs) {
  bfm_index_t *block_heights, *block_widths_first, *block_widths_second;
  bfm_index_t i, k;
  double *data_from_first, *data_from_second;

  head = recurse_d(head, order, input, nrows, ncols, nvecs, buffer, output,
                   &data_from_first, &data_from_second, 
                   &block_heights, &block_widths_first, &block_widths_second);

  /* Apply interpolation matrices */
  for (i = 0; i != order; ++i) {
    bfm_index_t n = block_widths_first[i] + block_widths_second[i];
    double in_buf[n * nvecs];
    /* Merge input vectors for this column */
    stack_vectors_d(data_from_first, block_widths_first[i] * nvecs,
                    data_from_second, block_widths_second[i] * nvecs,
                    in_buf);
    data_from_first += block_widths_first[i] * nvecs;
    data_from_second += block_widths_second[i] * nvecs;
      
    /* T_ip and T_k */
    head = apply_interpolation_d(head, in_buf, output, block_heights[2 * i], n, nvecs);
    output += block_heights[2 * i] * nvecs;
    /* B_ip and B_k */
    head = apply_interpolation_d(head, in_buf, output, block_heights[2 * i + 1], n, nvecs);
    output += block_heights[2 * i + 1] * nvecs;        
  }
  return head;
}

static INLINE char *apply_root_block_d(char *head,
                                       double *input, double *output,
                                       bfm_index_t m, bfm_index_t n,
                                       bfm_index_t nvecs) {
  bfm_index_t k = ((bfm_index_t*)head)[0];
  double buf[(k + 1) * nvecs];
  head += sizeof(bfm_index_t);
  head = apply_interpolation_d(head, input, buf, k, n, nvecs);
  head = skip_padding(head);
  dgemm_rrr((double*)head, buf, output, m, nvecs, k, 0.0);
  head += sizeof(double[m * k]);
  return head;
}



int bfm_apply_d(char *head, double *x, double *y,
                bfm_index_t nrows, bfm_index_t ncols, bfm_index_t nvecs) {
  bfm_index_t order, *block_heights, *block_widths_first, *block_widths_second;
  double *data_from_first, *data_from_second;
  bfm_index_t i;
  double *buffer = NULL, *buffer2 = NULL;
 /* TODO: Make sure this is sufficient. */
  bfm_index_t maxrowscols = (nrows > ncols) ? nrows : ncols;
  check((size_t)head % 16 == 0, "'head'is unaligned");
  check((size_t)x % 16 == 0, "'x' is unaligned");
  check((size_t)y % 16 == 0, "'y' is unaligned");
  checkf(((bfm_index_t*)head)[1] == nrows, "Expected nrows==%d but got %d", 
         ((bfm_index_t*)head)[1], nrows);
  checkf(((bfm_index_t*)head)[2] == ncols, "Expected ncols==%d but got %d", 
         ((bfm_index_t*)head)[2], ncols);
  //  printf("nrows=%d ncols=%d\n", nrows, ncols);
  buffer = memalign(16, sizeof(double[nrows * nvecs * 2]));
  buffer2 = memalign(16, sizeof(double[nrows * nvecs * 2]));

  order = ((bfm_index_t*)head)[0];
  head += sizeof(bfm_index_t[3]);
  assert(order >= 1);
  head = recurse_d(head, order, x, nrows, ncols, nvecs, buffer, buffer2,
                   &data_from_first, &data_from_second, 
                   &block_heights, &block_widths_first, &block_widths_second);

  /* Now, apply interpolation matrices as well as final dense diagonal blocks */
  for (i = 0; i != order; ++i) {
    bfm_index_t n = block_widths_first[i] + block_widths_second[i];
    double in_buf[n * nvecs];
    /* Merge input vectors for this column */
    stack_vectors_d(data_from_first, block_widths_first[i] * nvecs,
                    data_from_second, block_widths_second[i] * nvecs,
                    in_buf);
    data_from_first += block_widths_first[i] * nvecs;
    data_from_second += block_widths_second[i] * nvecs;
      
    /* T_ip and T_k */
    head = apply_root_block_d(head, in_buf, y, block_heights[2 * i], n, nvecs);
    y += block_heights[2 * i] * nvecs;
    nrows -= block_heights[2 * i] * nvecs;
    /* B_ip and B_k */
    head = apply_root_block_d(head, in_buf, y, block_heights[2 * i + 1], n, nvecs);
    y += block_heights[2 * i + 1] * nvecs;        
    nrows -= block_heights[2 * i + 1] * nvecs;
  }
  free(buffer);
  free(buffer2);
  return 0;
}



bfm_plan *bfm_create_plan(size_t k_max, size_t nblocks_max, size_t nvecs) {
  bfm_plan *plan;
  size_t i;
  if (k_max == 0) k_max++;
  plan = malloc(sizeof(bfm_plan));
  plan->k_max = k_max;
  plan->nblocks_max = nblocks_max;
  plan->nvecs = nvecs;
  plan->chunks_allocated = plan->chunk_stack_size = nblocks_max + 2;
  plan->vector_chunk_stack = malloc(sizeof(void*[plan->chunk_stack_size]));
  for (i = 0; i != plan->chunks_allocated; ++i) {
    plan->vector_chunk_stack[i] = memalign(BUF_ALIGN, sizeof(double[k_max * nvecs]));
  }
  plan->y_buf = memalign(BUF_ALIGN, sizeof(double[2 * k_max * nvecs]));
  return plan;
}

void bfm_destroy_plan(bfm_plan *plan) {
  int i;
  if (!plan) return;
  assert(plan->chunk_stack_size == plan->chunks_allocated);
  for (i = 0; i != plan->chunk_stack_size; ++i) {
    free(plan->vector_chunk_stack[i]);
  }
  free((double *)plan->vector_chunk_stack);
  free(plan->y_buf);
  free(plan);
}


struct _bfm_transpose_apply_context {
  bfm_plan *plan;
  pull_func_t pull_func;
  push_func_t push_func;
  void *caller_ctx;
  char *matrix_data;
  char **node_heap;
  int add_push;
};

static inline double *acquire_vector_chunk(bfm_plan *plan) {
  assert(plan->chunk_stack_size > 0);
  size_t idx = --plan->chunk_stack_size;
  double *r = plan->vector_chunk_stack[idx];
  plan->vector_chunk_stack[idx] = NULL;
  return r;
}

static inline double *release_vector_chunk(bfm_plan *plan, double *chunk) {
  assert(plan->chunk_stack_size < plan->chunks_allocated);
  plan->vector_chunk_stack[plan->chunk_stack_size++] = chunk;
}

static inline bfm_index_t read_index(char **head) {
  bfm_index_t result = *(bfm_index_t*)*head;
  *head += sizeof(bfm_index_t);
  return result;
}

static void read_interpolation_block(char **head, char **mask, double **interpolant,
                                     size_t n, size_t k) {
  *mask = *head;
  *head += sizeof(char[n]);
  *head = skip_padding(*head);
  *interpolant = (double*)*head;
  *head += sizeof(double[(n - k) * k]);
}

static void transpose_apply_interpolation_block(
                         char **head, double *output_left, double *output_right,
                         double *input, double *y_buf,
                         size_t n_left, size_t n_right, size_t k,
                         size_t nvecs, int should_add) {
  char *mask;
  double *interpolant;
  size_t n = n_left + n_right;
  read_interpolation_block(head, &mask, &interpolant, n, k);
  if (!should_add) {
    bfm_scatter_complement(mask, output_left, output_right, input, n_left, n_right, nvecs);
    dgemm_crc(input, interpolant, y_buf, nvecs, n - k, k, 0.0);
    bfm_scatter(mask, output_left, output_right, y_buf, n_left, n_right, nvecs);
  } else {
    bfm_scatter_complement_add(mask, output_left, output_right, input, n_left, n_right, nvecs);
    dgemm_crc(input, interpolant, y_buf, nvecs, n - k, k, 0.0);
    bfm_scatter_add(mask, output_left, output_right, y_buf, n_left, n_right, nvecs);
  }
}

static size_t transpose_apply_node(bfm_transpose_apply_context *ctx,
                                   size_t inode,
                                   size_t target_start,
                                   double **input_blocks) {
  char *node_data = ctx->node_heap[inode];
  size_t nblocks = read_index(&node_data);

  int is_root = (input_blocks == NULL);
  bfm_plan *plan = ctx->plan;
  double *input_buf;

  if (is_root) {
    assert(target_start == 0);
  }

  if (nblocks == 0) {
    /* Leaf node, simply push buffer to output vectors */
    double *input_buf;
    size_t n = read_index(&node_data);
    if (is_root) {
      input_buf = acquire_vector_chunk(plan);
      ctx->pull_func(input_buf, 0, n, plan->nvecs, ctx->caller_ctx);
    } else {
      input_buf = input_blocks[0];
    }
    ctx->push_func(input_buf, target_start, target_start + n, plan->nvecs,
                   ctx->add_push, ctx->caller_ctx);
    release_vector_chunk(plan, input_buf);
    return target_start + n;
  } else {
    bfm_index_t *block_heights = (bfm_index_t*)node_data;
    node_data += sizeof(bfm_index_t[nblocks]);
    char *left_child_data = ctx->node_heap[2 * inode];
    char *right_child_data = ctx->node_heap[2 * inode + 1];
    size_t nleft = read_index(&left_child_data);
    size_t nright = read_index(&right_child_data);
    assert((nleft == 0 && nright == 0) || 
           (nleft == nblocks / 2 && nright == nblocks / 2));
    bfm_index_t *left_child_block_heights = (bfm_index_t*)left_child_data;
    bfm_index_t *right_child_block_heights = (bfm_index_t*)right_child_data;
    
    /* Process interpolation nodes. */
    size_t input_pos = 0; /* only used if is_root */
    double *left_output_blocks[nblocks / 2], *right_output_blocks[nblocks / 2];
    for (size_t i = 0; i != nblocks; i += 2) {
      double *output_left = acquire_vector_chunk(plan);
      double *output_right = acquire_vector_chunk(plan);
      size_t k_T = block_heights[i];
      size_t k_B = block_heights[i + 1];
      size_t n_left = left_child_block_heights[i / 2];
      size_t n_right = right_child_block_heights[i / 2];
      char *mask;
      double *interpolant;

      assert(k_T <= plan->k_max && k_B <= plan->k_max);

      /* Process T */
      if (is_root) {
        input_buf = acquire_vector_chunk(plan);
        ctx->pull_func(input_buf, input_pos, input_pos + k_T, plan->nvecs, ctx->caller_ctx);
        input_pos += k_T;
      } else {
        input_buf = input_blocks[i];
        input_blocks[i] = NULL;
      }
      transpose_apply_interpolation_block(&node_data, output_left, output_right,
                                          input_buf, plan->y_buf,
                                          n_left, n_right, k_T, plan->nvecs, 0);
      release_vector_chunk(plan, input_buf);

      /* Process B */
      if (is_root) {
        input_buf = acquire_vector_chunk(plan);
        ctx->pull_func(input_buf, input_pos, input_pos + k_B, plan->nvecs, ctx->caller_ctx);
        input_pos += k_B;
      } else {
        input_buf = input_blocks[i + 1];
        input_blocks[i + 1] = NULL;
      }
      transpose_apply_interpolation_block(&node_data, output_left, output_right,
                                          input_buf, plan->y_buf,
                                          n_left, n_right, k_B, plan->nvecs, 1);
      release_vector_chunk(plan, input_buf);
      left_output_blocks[i / 2] = output_left;
      right_output_blocks[i / 2] = output_right;
    }
    
    /* Recurse */
    size_t target_stop = 
      transpose_apply_node(ctx, 2 * inode, target_start, left_output_blocks);
    target_stop = 
      transpose_apply_node(ctx, 2 * inode + 1, target_stop, right_output_blocks);
    
    return target_stop;
  }
}

int bfm_transpose_apply_d(bfm_plan *plan,
                          char *matrix_data,
                          size_t nrows, 
                          size_t ncols,
                          pull_func_t pull_func,
                          push_func_t push_func,
                          void *caller_ctx) {
  bfm_transpose_apply_context ctx;
  char *head = matrix_data;
  ctx.caller_ctx = caller_ctx;
  ctx.pull_func = pull_func;
  ctx.push_func = push_func;
  ctx.matrix_data = matrix_data;
  ctx.plan = plan;
  check((size_t)matrix_data % 16 == 0, "matrix_data not 128-bit aligned");
  
  /* Read in tree. Shape of the S-matrix forest is determined by the
     start level and stop level. Offsets to each node is stored in a
     heap structure that is present; we translate it to pointers in
     memory.

     The heap base pointer is the virtual zero level, i.e., it
     incorporates the subtraction of the first index.
  */
  int32_t *header_fields = (int32_t*)head; 
  checkf(header_fields[0] == nrows, "Data says nrows==%d, caller says nrows==%d",
         header_fields[0], (int)nrows);
  checkf(header_fields[1] == ncols, "Data says ncols==%d, caller says ncols==%d",
         header_fields[1], (int)ncols);
  size_t first_level_size = header_fields[2];
  size_t heap_size = ((int32_t*)head)[3];
  size_t node_heap_first_index = ((int32_t*)head)[4];
  /* One padding */
  head += sizeof(int32_t[6]);

  char *heap_buf[heap_size];
  ctx.node_heap = heap_buf - node_heap_first_index;

  for (size_t i = 0; i != heap_size; ++i) {
    heap_buf[i] = matrix_data + ((int64_t*)head)[i];
  }

  /* Start to apply the root level. 

     We signal whether we're in the first iteration using the add_push
     flag, which the caller can inspect and (optionally) zero the output buffer.
  */
  ctx.add_push = 0; /* */
  for (size_t inode = node_heap_first_index;
       inode != node_heap_first_index + first_level_size;
       ++inode) {

    size_t stop = transpose_apply_node(&ctx, inode, 0, NULL);
    assert(stop == ncols);
    ctx.add_push = 1; /* Any remaining root nodes require addition. */
  }
  
  return 0;
}
