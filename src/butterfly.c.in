#undef NDEBUG
#include <stdlib.h>
#include <stdio.h>
#include <malloc.h>
#include <assert.h>
#include <xmmintrin.h>
#include <emmintrin.h>
#include <errno.h>
#include "butterfly.h"
#include "blas.h"
#include "fastsht_error.h"
#include "butterfly_utils.h"

/* Common page size on AMD64 */
#define BUF_ALIGN 4096

typedef __m128d m128d;
typedef __m128i m128i;
typedef __m128 m128;
typedef union {
  char c[16];
  m128i v;
} v16c;

/*
Spread and merge
*/

{{py:
nvecs_specs = [2, None]

nvecarg_specs = [', size_t nvecs' if x is None else '' for x in nvecs_specs]
nvecs_instances = zip(
    nvecs_specs, # nvecs
    ['_%d' % x if x is not None else '' for x in nvecs_specs], # suffix
    [', size_t nvecs' if x is None else '' for x in nvecs_specs],     # trailing_args
    ['#define nvecs %d' % x if x is not None else '' for x in nvecs_specs], # define
    ['#undef nvecs' if x is not None else '' for x in nvecs_specs]) # undef

}}


{{for nvecs, suffix, trailing_args, define, undefine in nvecs_instances}}
static char *merge_vectors{{suffix}}(char *mask, double *x, double *a, double *b,
                           int32_t alen, int32_t blen{{trailing_args}}) {
  {{define}}
  int j;
  char *end = mask + alen + blen;
  int m;
  m128d val;
  assert(nvecs % 2 == 0);
  assert((size_t)x % 16 == 0);
  assert((size_t)a % 16 == 0);
  assert((size_t)b % 16 == 0);
  while (mask != end) {
    m = 2 * *mask++;
    for (j = 0; j != nvecs / 2; ++j) {
      val = _mm_load_pd(x);
      x += 2;
      _mm_store_pd(a, val);
      _mm_store_pd(b, val);
      a += (2 - m);
      b += m;
    }
  }
  return mask;
  {{undefine}}
}
{{endfor}}



/*
Copies 'a' to the locations marked with 0 in the mask,
and 'b' to the locations marked with 1.

The target buffer is the concatenation of target1 and target2.
The lengths of each of the buffer parts is in units of nvecs.
*/
{{py:
def scatter_name(group, should_add, nvecs):
    return 'bfm_scatter_group%d_%s%s' % (
        group,
        'add' if should_add else 'noadd',
        '_%d' % nvecs if nvecs else ''
        )
}}

{{for group in [0, 1]}}
{{for nvecs, suffix, trailing_args, define, undefine in nvecs_instances}}
{{for should_add in [False, True]}}
const char *{{scatter_name(group, should_add, nvecs)}}(
    const char *restrict mask, 
    double *restrict target1,
    double *restrict target2,
    const double *restrict source,
    size_t len1, size_t len2{{trailing_args}}) {
  /* Time spent is this routine is likely due to cache effects...be
     sure to profile on in-cache data.

     This implementation appears to be rather fast; tried playing with
     eliminating branches through bit operations etc. and it did not give
     speedups.  Best way forward to speed this up is likely to pack
     more information into each char in the mask, and use a switch to
     dispatch to multiple load/stores. */
  {{define}}
  int j;
  const char *restrict end;
  int m;
  int x = 0;
  m128d val;
  {{if should_add}}m128d tmp;{{endif}}
  assert(nvecs % 2 == 0);
  assert((size_t)target1 % 16 == 0);
  assert((size_t)target2 % 16 == 0);
  assert((size_t)source % 16 == 0);
  {{for idx in [1, 2]}}
  end = mask + len{{idx}};
  while (mask != end) {
    m = *mask++;
    if (m == {{group}}) {
      for (j = 0; j != nvecs / 2; ++j) {
        val = _mm_load_pd(source);
        source += 2;
        {{if should_add}}
        tmp = _mm_load_pd(target{{idx}});
        val = _mm_add_pd(val, tmp);
        {{endif}}
        _mm_store_pd(target{{idx}}, val);
        target{{idx}} += 2;
      }
    } else {
      target{{idx}} += nvecs;
    }
    x++;
  }
  {{endfor}}
  return mask;
  {{undefine}}
}
{{endfor}}
{{endfor}}
{{endfor}}

/* Runtime dispatcher for scatter */
const char *bfm_scatter(
    const char *restrict mask, 
    double *restrict target1,
    double *restrict target2,
    const double *restrict source,
    size_t len1, size_t len2, size_t nvecs,
    int group, int should_add) {
  {{py: args='mask, target1, target2, source, len1, len2'}}
  {{for group in [0, 1]}}
  {{for should_add in [False, True]}}
  if ((group == {{group}}) && ({{'' if should_add else '!'}}should_add))
 {
   if (0) {
    {{for nvecs in [x for x in nvecs_specs if x is not None]}}
    if (nvecs == {{nvecs}}) {
      return {{scatter_name(group, should_add, nvecs)}}({{args}});
    }
    {{endfor}}
   }    else {
      return {{scatter_name(group, should_add, None)}}({{args}}, nvecs);
    }
  }
  {{endfor}}
  {{endfor}}
  check(0, "bfm_scatter: Invalid group or should_add argument");
}


/*
Utils
*/

static void print_array(char *msg, double* arr, bfm_index_t len) {
  bfm_index_t i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}


/*
Type implementations
*/


/*static int dense_rowmajor_right_d(char *matrixdata, double *x, double *y,
                                  bfm_index_t nrow, bfm_index_t ncol, bfm_index_t nvec) {
  double *matrix = (double*)(matrixdata + 16);
  /* dgemm uses Fortran-order, so do transposed multiply;
     C-order: y^T = x^T * matrix^T
     Fortran-order: y = x * matrix
/
  int m = nvec, n = nrow, k = ncol;
  dgemm('N', 'N', m, n, k, 1.0, x, m, matrix, k, 0.0, y, m);
  return 0;
}*/


static void stack_vectors_d(double *a, bfm_index_t alen, double *b, bfm_index_t blen,
                            double *output) {
  bfm_index_t i;
  for (i = 0; i != alen; ++i) {
    output[i] = a[i];
  }
  for (i = 0; i != blen; ++i) {
    output[alen + i] = b[i];
  }
}

/*
  Filter input x into two parts: Those hit with identity matrix which goes
  to y, and those going to a set of temporary vectors tmp_vecs which
  will be multiplied with the interpolation matrix and then added to y.
  
  input is n-by-nvec, output is k-by-nvec
*/
volatile char globchar;
static void access_mem(char *head, size_t size) {
  char r = 0;
  int i;
  for (i = 0; i < size; i += 64) {
    r += head[i];
  }
  globchar = r;
}

static char *apply_interpolation_d(char *head, double *input, double *output,
                                   int32_t k, int32_t n, int32_t nvec) {
  double tmp_vecs[nvec * (n - k + 1)];
  if (nvec == 2) {
    head = merge_vectors_2(head, input, output, tmp_vecs, k, n - k);
  } else {
    head = merge_vectors(head, input, output, tmp_vecs, k, n - k, nvec);
  }
  head = skip_padding(head);
  access_mem(head, k * (n - k));
  dgemm_rrr((double*)head, tmp_vecs, output, k, nvec, n - k, 1.0);
  head += sizeof(double[k * (n - k)]);
  return head;
}

/* Forward declaration */
static char *apply_butterfly_node_d(char *head, bfm_index_t order, double *input,
                                    double *output, double *buffer,
                                    bfm_index_t nrows, bfm_index_t ncols,
                                    bfm_index_t nvecs);

static INLINE char *recurse_d(char *head, bfm_index_t order,
                              double *input, bfm_index_t nrows, 
                              bfm_index_t ncols,
                              bfm_index_t nvecs,
                              double *output,
                              double *buffer2,
                              double **data_from_first,
                              double **data_from_second,
                              bfm_index_t **out_block_heights,
                              bfm_index_t **out_block_widths_first,
                              bfm_index_t **out_block_widths_second) {
  printf("%d %d\n", *(bfm_index_t*)head, order);
  assert(*(bfm_index_t*)head == 2 * order);   /* nblocks field */
  head += sizeof(bfm_index_t);
  *out_block_heights = (bfm_index_t*)head; /* [2 * order] */
  bfm_index_t nrows_first = ((bfm_index_t*)head)[2 * order];
  bfm_index_t nrows_second = ((bfm_index_t*)head)[2 * order + 1];
  bfm_index_t col_split = ((bfm_index_t*)head)[2 * order + 2];
  /*checkf(nrows >= nrows_first + nrows_second,
         "nrows=%d, but nrows_first + nrows_second = %d + %d = %d",
         nrows, nrows_first, nrows_second, nrows_first + nrows_second);*/
  head += sizeof(bfm_index_t[2 * order + 3]);
  if (order == 1) {
    /* Parse the two leaf node identity matrices */
    bfm_index_t *ihead = (bfm_index_t*)head;
    assert((ihead[0] == ihead[2]) && (ihead[0] == 0));
    printf("%d %d %d\n", ihead[0], ihead[2], order);
    *out_block_widths_first = ihead + 1;
    *out_block_widths_second = ihead + 3;
    head += sizeof(bfm_index_t[4]);
    nrows_first = (*out_block_widths_first)[0];
    *data_from_first = input;
    *data_from_second = *data_from_first + nrows_first * nvecs;
  } else {
    //    printf("IN\n");
    *data_from_first = output;
    /* Recurse to sub-nodes. */
    *out_block_widths_first = (bfm_index_t*) head;
    //    printf("CONSUMING %d : %lx\n", nrows_first, (size_t)output);
    head = apply_butterfly_node_d(head, order / 2, input, *data_from_first, buffer2, nrows_first,
                                  col_split, nvecs);
    input += col_split * nvecs;
    *data_from_second = *data_from_first + nrows_first * nvecs;
    *out_block_widths_second = (bfm_index_t*) head;
    //printf("CONSUMING %d : %lx\n", nrows_second, (size_t)output);
    head = apply_butterfly_node_d(head, order / 2, input, *data_from_second, buffer2, nrows_second,
                                  ncols - col_split, nvecs);
    //printf("OUT\n");
  }
  return head;
}

static char *apply_butterfly_node_d(char *head, bfm_index_t order, double *input,
                                    double *output, double *buffer,
                                    bfm_index_t nrows, bfm_index_t ncols,
                                    bfm_index_t nvecs) {
  bfm_index_t *block_heights, *block_widths_first, *block_widths_second;
  bfm_index_t i;
  double *data_from_first, *data_from_second;

  head = recurse_d(head, order, input, nrows, ncols, nvecs, buffer, output,
                   &data_from_first, &data_from_second, 
                   &block_heights, &block_widths_first, &block_widths_second);

  /* Apply interpolation matrices */
  for (i = 0; i != order; ++i) {
    bfm_index_t n = block_widths_first[i] + block_widths_second[i];
    double in_buf[n * nvecs];
    /* Merge input vectors for this column */
    stack_vectors_d(data_from_first, block_widths_first[i] * nvecs,
                    data_from_second, block_widths_second[i] * nvecs,
                    in_buf);
    data_from_first += block_widths_first[i] * nvecs;
    data_from_second += block_widths_second[i] * nvecs;
      
    /* T_ip and T_k */
    head = apply_interpolation_d(head, in_buf, output, block_heights[2 * i], n, nvecs);
    output += block_heights[2 * i] * nvecs;
    /* B_ip and B_k */
    head = apply_interpolation_d(head, in_buf, output, block_heights[2 * i + 1], n, nvecs);
    output += block_heights[2 * i + 1] * nvecs;        
  }
  return head;
}

static INLINE char *apply_root_block_d(char *head,
                                       double *input, double *output,
                                       bfm_index_t m, bfm_index_t n,
                                       bfm_index_t nvecs) {
  bfm_index_t k = ((bfm_index_t*)head)[0];
  double buf[(k + 1) * nvecs];
  head += sizeof(bfm_index_t);
  head = apply_interpolation_d(head, input, buf, k, n, nvecs);
  head = skip_padding(head);
  dgemm_rrr((double*)head, buf, output, m, nvecs, k, 0.0);
  head += sizeof(double[m * k]);
  return head;
}



int bfm_apply_d(char *head, double *x, double *y,
                bfm_index_t nrows, bfm_index_t ncols, bfm_index_t nvecs) {
  bfm_index_t order, *block_heights, *block_widths_first, *block_widths_second;
  double *data_from_first, *data_from_second;
  bfm_index_t i;
  double *buffer = NULL, *buffer2 = NULL;
 /* TODO: Make sure this is sufficient. */
  check((size_t)head % 16 == 0, "'head'is unaligned");
  check((size_t)x % 16 == 0, "'x' is unaligned");
  check((size_t)y % 16 == 0, "'y' is unaligned");
  checkf(((bfm_index_t*)head)[1] == nrows, "Expected nrows==%d but got %d", 
         ((bfm_index_t*)head)[1], nrows);
  checkf(((bfm_index_t*)head)[2] == ncols, "Expected ncols==%d but got %d", 
         ((bfm_index_t*)head)[2], ncols);
  //  printf("nrows=%d ncols=%d\n", nrows, ncols);
  buffer = memalign(16, sizeof(double[nrows * nvecs * 2]));
  buffer2 = memalign(16, sizeof(double[nrows * nvecs * 2]));

  order = ((bfm_index_t*)head)[0];
  head += sizeof(bfm_index_t[3]);
  assert(order >= 1);
  head = recurse_d(head, order, x, nrows, ncols, nvecs, buffer, buffer2,
                   &data_from_first, &data_from_second, 
                   &block_heights, &block_widths_first, &block_widths_second);

  /* Now, apply interpolation matrices as well as final dense diagonal blocks */
  for (i = 0; i != order; ++i) {
    bfm_index_t n = block_widths_first[i] + block_widths_second[i];
    double in_buf[n * nvecs];
    /* Merge input vectors for this column */
    stack_vectors_d(data_from_first, block_widths_first[i] * nvecs,
                    data_from_second, block_widths_second[i] * nvecs,
                    in_buf);
    data_from_first += block_widths_first[i] * nvecs;
    data_from_second += block_widths_second[i] * nvecs;
      
    /* T_ip and T_k */
    head = apply_root_block_d(head, in_buf, y, block_heights[2 * i], n, nvecs);
    y += block_heights[2 * i] * nvecs;
    nrows -= block_heights[2 * i] * nvecs;
    /* B_ip and B_k */
    head = apply_root_block_d(head, in_buf, y, block_heights[2 * i + 1], n, nvecs);
    y += block_heights[2 * i + 1] * nvecs;        
    nrows -= block_heights[2 * i + 1] * nvecs;
  }
  free(buffer);
  free(buffer2);
  return 0;
}



bfm_plan *bfm_create_plan(size_t k_max, size_t nblocks_max, size_t nvecs,
                          sem_t *mem_semaphore, sem_t *cpu_semaphore) {
  bfm_plan *plan;
  size_t i;
  if (k_max == 0) k_max++;
  plan = malloc(sizeof(bfm_plan));
  plan->k_max = k_max;
  plan->nblocks_max = nblocks_max;
  plan->nvecs = nvecs;
  plan->mem_semaphore = mem_semaphore;
  plan->cpu_semaphore = cpu_semaphore;
  plan->chunks_allocated = plan->chunk_stack_size = nblocks_max + 2;
  plan->vector_chunk_stack = malloc(sizeof(void*[plan->chunk_stack_size]));
  for (i = 0; i != plan->chunks_allocated; ++i) {
    plan->vector_chunk_stack[i] = memalign(BUF_ALIGN, sizeof(double[k_max * nvecs]));
  }
  plan->y_buf = memalign(BUF_ALIGN, sizeof(double[2 * k_max * nvecs]));
  return plan;
}

void bfm_destroy_plan(bfm_plan *plan) {
  int i;
  if (!plan) return;
  assert(plan->chunk_stack_size == plan->chunks_allocated);
  for (i = 0; i != plan->chunk_stack_size; ++i) {
    free(plan->vector_chunk_stack[i]);
  }
  free((double *)plan->vector_chunk_stack);
  free(plan->y_buf);
  free(plan);
}


struct _bfm_transpose_apply_context {
  bfm_plan *plan;
  pull_func_t pull_func;
  void *caller_ctx;
  char *matrix_data;
  char **node_heap;
  char **residual_payload_headers;
  double *target;
  int current_root_idx;
  int add_push;
};

static inline double *acquire_vector_chunk(bfm_plan *plan) {
  assert(plan->chunk_stack_size > 0);
  size_t idx = --plan->chunk_stack_size;
  double *r = plan->vector_chunk_stack[idx];
  plan->vector_chunk_stack[idx] = NULL;
  return r;
}

static inline void release_vector_chunk(bfm_plan *plan, double *chunk) {
  assert(plan->chunk_stack_size < plan->chunks_allocated);
  plan->vector_chunk_stack[plan->chunk_stack_size++] = chunk;
}

static void read_interpolation_block(char **head, char **mask, double **interpolant,
                                     size_t n, size_t k) {
  *mask = *head;
  *head += sizeof(char[n]);
  *head = skip_padding(*head);
  *interpolant = (double*)*head;
  *head += sizeof(double[(n - k) * k]);
}

static void transpose_apply_interpolation_block(
                         char **head, double *output_left, double *output_right,
                         double *input, double *y_buf,
                         size_t n_left, size_t n_right, size_t k,
                         size_t nvecs, int should_add) {
  char *mask;
  double *interpolant;
  size_t n = n_left + n_right;
  read_interpolation_block(head, &mask, &interpolant, n, k);

  bfm_scatter(mask, output_left, output_right, input, n_left, n_right, nvecs, 0, should_add);
  dgemm_ccc(input, interpolant, y_buf, nvecs, n - k, k, 0.0);
  bfm_scatter(mask, output_left, output_right, y_buf, n_left, n_right, nvecs, 1, should_add);
}

static void copy_vectors(double *input, double *target, size_t target_start, size_t target_stop,
                         size_t nvecs, int should_add) {
  size_t i;
  size_t n = (target_stop - target_start) * nvecs;
  target += target_start * nvecs;
  if (should_add) {
    for (i = 0; i != n; ++i) {
      target[i] += input[i];
    }
  } else {
    for (i = 0; i != n; ++i) {
      target[i] = input[i];
    }
  }
}

#if 0

static void enter_mem_section(bfm_plan *plan) {}
static void exit_mem_section(bfm_plan *plan) {}

#else

static void enter_mem_section(bfm_plan *plan) {
  int r;
  while (1) {
    while ((r = sem_trywait(plan->mem_semaphore)) == EINTR) {};
    if (r == 0) {
      /* Got memory */
      return;
    } else {
    /* No memory lock available right away, yield the CPU and
       reacquire it in order to give CPU-bound threads a chance to take
       over. */
    sem_post(plan->cpu_semaphore);
    while ((r = sem_wait(plan->cpu_semaphore)) == EINTR) {};
    }
  }
}

static void exit_mem_section(bfm_plan *plan) {
  sem_post(plan->mem_semaphore);
}

#endif

static size_t transpose_apply_node(bfm_transpose_apply_context *ctx,
                                   size_t inode,
                                   size_t target_start,
                                   double **input_blocks) {
  char *node_data = ctx->node_heap[inode];
  size_t nblocks = read_index(&node_data);

  int is_root = (input_blocks == NULL);
  bfm_plan *plan = ctx->plan;
  double *input_block;

  char *payloads[((nblocks == 0) ? 1 : nblocks) + 1];

  if (is_root) {
    char *payload_head = ctx->residual_payload_headers[ctx->current_root_idx];
    size_t n = read_int64(&payload_head);
    assert((n == nblocks) || (nblocks == 0 && n == 1));
    read_pointer_list(&payload_head, payloads, n + 1, ctx->matrix_data);
  }

  if (nblocks == 0) {
    /* Leaf node, simply push buffer to output vectors. */
    size_t n = read_index(&node_data);
    size_t target_stop = target_start + n;
    if (is_root) {
      /* Simply stream data straight through */
      assert(!ctx->add_push); /* TODO Do something sane */
      exit_mem_section(ctx->plan);
      ctx->pull_func(ctx->target, target_start, target_stop, plan->nvecs, payloads[0],
                     payloads[1] - payloads[0], ctx->caller_ctx);
      enter_mem_section(ctx->plan);
    } else {
      /* TODO: Avoid this copy at leafs, go straight to target in the nblocks=2 case */
      input_block = input_blocks[0];
      copy_vectors(input_block, ctx->target, target_start, target_stop, plan->nvecs,
                   ctx->add_push);
      release_vector_chunk(plan, input_block);
    }
    return target_stop;
  } else {
    bfm_index_t *block_heights = (bfm_index_t*)node_data;
    node_data += sizeof(bfm_index_t[nblocks]);
    char *left_child_data = ctx->node_heap[2 * inode];
    char *right_child_data = ctx->node_heap[2 * inode + 1];
    size_t nleft = read_index(&left_child_data);
    size_t nright = read_index(&right_child_data);
    assert((nleft == 0 && nright == 0) || 
           (nleft == nblocks / 2 && nright == nblocks / 2));
    bfm_index_t *left_child_block_heights = (bfm_index_t*)left_child_data;
    bfm_index_t *right_child_block_heights = (bfm_index_t*)right_child_data;

    /* Process interpolation nodes. */
    size_t input_pos = 0; /* only used if is_root */
    double *out_left_list[nblocks / 2], *out_right_list[nblocks / 2];
    for (size_t i = 0; i != nblocks / 2; ++i) {
      double *out_left = out_left_list[i] = acquire_vector_chunk(plan);
      double *out_right = out_right_list[i] = acquire_vector_chunk(plan);
      size_t n_left = left_child_block_heights[i];
      size_t n_right = right_child_block_heights[i];
      assert(n_left <= plan->k_max && n_right <= plan->k_max);
      
      /* Loop over cases T and B */
      for (int j = 0; j != 2; ++j) {
        size_t k = block_heights[2 * i + j];
        assert(k <= plan->k_max);
        if (is_root) {
          input_block = acquire_vector_chunk(plan);
          exit_mem_section(ctx->plan);
          ctx->pull_func(input_block, input_pos, input_pos + k,
                         plan->nvecs, payloads[2 * i + j],
                         payloads[2 * i + j + 1] - payloads[2 * i + j],
                         ctx->caller_ctx);
          enter_mem_section(ctx->plan);
          input_pos += k;
        } else {
          input_block = input_blocks[2 * i + j];
          input_blocks[2 * i + j] = NULL;
        }
        int should_add = j;
        transpose_apply_interpolation_block(&node_data, out_left, out_right,
                                            input_block, plan->y_buf,
                                            n_left, n_right, k, plan->nvecs,
                                            should_add);
        release_vector_chunk(plan, input_block);
      }
    }
    
    /* Recurse */
    size_t idx = target_start;
    idx = transpose_apply_node(ctx, 2 * inode, idx, out_left_list);
    idx = transpose_apply_node(ctx, 2 * inode + 1, idx, out_right_list);
    return idx;
  }
}

int bfm_transpose_apply_d(bfm_plan *plan,
                          char *matrix_data,
                          pull_func_t pull_func,
                          double *target,
                          size_t target_len,
                          void *caller_ctx) {
  bfm_transpose_apply_context ctx;
  bfm_matrix_data_info info;
  char *head = matrix_data;
  ctx.caller_ctx = caller_ctx;
  ctx.pull_func = pull_func;
  ctx.matrix_data = matrix_data;
  ctx.target = target;
  ctx.plan = plan;
  check((size_t)matrix_data % 16 == 0, "matrix_data not 128-bit aligned");
  
  sem_wait(plan->cpu_semaphore);
  enter_mem_section(plan);

  /* Read in tree. Shape of the S-matrix forest is determined by the
     start level and stop level. Offsets to each node is stored in a
     heap structure that is present; we translate it to pointers in
     memory.

     The heap base pointer is the virtual zero level, i.e., it
     incorporates the subtraction of the first index.
  */
  head = bfm_query_matrix_data(head, &info);

  check(target_len == plan->nvecs * info.ncols, "target_len does not match ncols * nvecs");


  /* Read residual matrix payload data */
  char *residual_payload_headers[info.first_level_size];
  ctx.residual_payload_headers = residual_payload_headers;
  read_pointer_list(&head, residual_payload_headers, info.first_level_size, matrix_data);

  /* Read tree node pointers in binary heap structure */
  char *heap_buf[info.heap_size];
  ctx.node_heap = heap_buf - info.heap_first_index;

  read_pointer_list(&head, heap_buf, info.heap_size, matrix_data);


  /* Start to apply the root level. 

     We signal whether we're in the first iteration using the add_push
     flag, which the caller can inspect and (optionally) zero the output buffer.
  */
  ctx.add_push = 0;
  ctx.current_root_idx = 0;
  size_t start = 0;
  for (size_t inode = info.heap_first_index;
       inode != info.heap_first_index + info.first_level_size;
       ++inode) {
    start = transpose_apply_node(&ctx, inode, start, NULL);
    ctx.current_root_idx++;
  }
  assert(start == info.ncols);
  exit_mem_section(plan);
  sem_post(plan->cpu_semaphore);
  return 0;
}

char *bfm_query_matrix_data(char *head, bfm_matrix_data_info *info) {
  info->nrows = read_int32(&head);
  info->ncols = read_int32(&head);
  info->k_max = read_int32(&head);
  info->nblocks_max = read_int32(&head);
  info->element_count = read_int64(&head);
  info->first_level_size = read_int32(&head);
  info->heap_size = read_int32(&head);
  info->heap_first_index = read_int32(&head);
  read_int32(&head);
  return head;
}
