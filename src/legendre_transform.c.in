#undef NDEBUG
#include <assert.h>
#include <xmmintrin.h>
#include <emmintrin.h>
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <stdint.h>

#include "fastsht_error.h"
#include "legendre_transform.h"

#ifndef INLINE
# if __STDC_VERSION__ >= 199901L
#  define INLINE inline
# else
#  define INLINE
# endif
#endif

typedef __m128d m128d;
typedef __m128 m128;

static void _printreg(char *msg, m128d r) {
  double *pd = (double*)&r;
  printf("%s = [%f %f]\n", msg, pd[0], pd[1]);
}

static INLINE size_t imin(size_t a, size_t b) {
  return (a < b) ? a : b;
}

#define printreg(x) _printreg(#x, x)

#define MULADD(z, a, b) _mm_add_pd(z, _mm_mul_pd(a, b))


{{for xchunksize in [1, 2, 6]}}
{{py:
single = xchunksize == 1
nregs = 1 if single else xchunksize // 2
}}
static void legendre_transform_chunk{{xchunksize}}_nvec2(size_t ix_start,
                                                         size_t ix_stop,
                                                         size_t nk,
                                                         double *a,
                                                         double *y,
                                                         double *x_squared,
                                                         double *auxdata,
                                                         double *P,
                                                         double *Pp1) {
  size_t i, k, s;
  assert((ix_stop - ix_start) % {{xchunksize}} == 0);

  /* We compute:

        y_{k,j} = sum_k  P_{k, i} a_{k,j}

     Overall strategy: Compute P with three-term recurrence relation
     in k and multiply results with a as we go. We compute with xchunksize=6
     columns of P at the time, which is what we can fit in the 16
     registers available without spilling. The shuffling&loading of
     'a' and auxiliary data (alpha, beta, gamma) is amortized over
     these 6 columns. 'x_squared' is streamed in again and again (once
     per row of P) to conserve registers -- letting NS=4 or spilling
     registers into stack were both significantly slower).

     The output 'y' sits in 6 SSE accumulation registers (12 values
     at the time) and is transposed and stored only after each
     loop over k.
    
  */

  auxdata -= 6; /* auxdata not passed for k=0, k=1 */

  /* Process strips (xchunksize={{xchunksize}}) */
  for (i = ix_start; i != ix_stop; i += {{xchunksize}}) {
    /* In comments and variable names we will assume that k starts on
       0 to keep things brief; however, this is a dynamic quantity to
       support ignoring near-zero parts of P. */
    k = 0;
    /* We loop over k in the inner-most loop and fully compute y_ij
       before storing it.

       If xchunksize == 1, computations on P_ki is duplicated on both
       register entries, and SSE is only used for the 'matrix multiplication'
       step:

         P_ki = [P_{k,i} P_{k,i}]  (same for Pp_ki, Ppp_ki)
         a_kj = [ a_{k,j}  a_{k,j+1} ]
         y_ij = [y_{i,j}  y_{i,j+1}]

       Otherwise, we process two strips at the time in P_ki. The
       accumulation registers y are kept in transposed order, and then
       transposed in the end:

         P_ki =   [P_{k,i} P_{k,i+1}] (same for Pp_ki, Ppp_ki)

         a_kj  = [ a_{k,j}  a_{k,j} ]
         a_kjp = [ a_{k,j+1}  a_{k,j+1} ]
    
         y_ij  = [y_{i,j}   y_{i+1, j}  ]
         y_ijp = [y_{i,j+1} y_{i+1,j+1} ]

    */
    m128d y_ij[{{nregs}}], a_kj;
    m128d P_ki[{{nregs}}], Pp_ki[{{nregs}}], Ppp_ki[{{nregs}}];
    {{if not single}}
    m128d y_ijp[{{nregs}}], a_kjp;
    {{endif}}


    /* Template for loading a_kj */
    {{def load_a}}
    {
      a_kj = _mm_load_pd(a + k * 2);
      {{if not single}}
        a_kjp = _mm_unpackhi_pd(a_kj, a_kj);
        a_kj = _mm_unpacklo_pd(a_kj, a_kj);
      {{endif}}
    }
    {{enddef}}

    /* Template for the core loop body

       Uses recurrence relation and does matmul for a strip of xchunksize
       elements.

       INPUT (C variables assumed to be present):
     
       alpha, beta, gamma:
         Auxiliary information, values duplicated across
         register elements.

       k, i:
         Counters set to row and column of P matrix. P[k, i:i + xchunksize]
         is processed.

       x_squared is streamed in as we go in order to conserve
       registers, so that we can let xchunksize == 6 instead of 4. The extra
       loads we spend here are worth it so that the loads&shuffles of
       alpha, beta, gamma, and a can be reused over 3 stripes instead
       of 2.
         
       This had a noticeable impact on performance, from around 75%
       of peak GFLOP to 85%.
    */
    {{def loop_body(xchunksize)}}
    {
      m128d w[{{nregs}}];
      for (s = 0; s != {{nregs}}; ++s) {
        {{if single}}
          ((double*)&w[s])[0] = x_squared[i + 2 * s];
          w[s] = _mm_unpacklo_pd(w[s], w[s]);
        {{else}}
          w[s] = _mm_load_pd(x_squared + i + 2 * s);
        {{endif}}
        w[s] = _mm_add_pd(w[s], alpha);
        w[s] = _mm_mul_pd(w[s], beta);
        w[s] = _mm_mul_pd(w[s], Pp_ki[s]);
      
        P_ki[s] = _mm_mul_pd(Ppp_ki[s], gamma);
        P_ki[s] = _mm_add_pd(P_ki[s], w[s]);
      
        Ppp_ki[s] = Pp_ki[s];
        Pp_ki[s] = P_ki[s];
      }
      {{load_a}}
      for (s = 0; s != {{nregs}}; ++s) {
        y_ij[s] = MULADD(y_ij[s], P_ki[s], a_kj);
        {{if not single}}
          y_ijp[s] = MULADD(y_ijp[s], P_ki[s], a_kjp);
        {{endif}}
      }
    }
    {{enddef}}


    /* Move on to computation.
       First two values of k needs special treatment as they are
       already computed (starting values). For the first k we
       initialize y_ij, and after that we accumulate in y_ij.
    */

    {{load_a}}
    {{if single}}
      Ppp_ki[0] = (m128d){ P[i], P[i] };
      y_ij[0] = _mm_mul_pd(Ppp_ki[0], a_kj);
    {{else}}
      for (s = 0; s != {{nregs}}; ++s) {
        Ppp_ki[s] = _mm_load_pd(P + i + 2 * s);
        y_ij[s] = _mm_mul_pd(Ppp_ki[s], a_kj);
        y_ijp[s] = _mm_mul_pd(Ppp_ki[s], a_kjp);
      }
    {{endif}}

    ++k;
    {{load_a}}

    {{if single}}
      Pp_ki[0] = (m128d){ Pp1[i], Pp1[i] };
      y_ij[0] = MULADD(y_ij[0], Pp_ki[0], a_kj);
    {{else}}
      for (s = 0; s != {{nregs}}; ++s) {
        Pp_ki[s] = _mm_load_pd(Pp1 + i + 2 * s);
        y_ij[s] = MULADD(y_ij[s], Pp_ki[s], a_kj);
        y_ijp[s] = MULADD(y_ijp[s], Pp_ki[s], a_kjp);
      }
    {{endif}}
    ++k;

    m128d aux1, aux2, aux3, alpha, beta, gamma;
    size_t loop_stop = nk - nk % 2;
    if (nk > 2 && (intptr_t)auxdata % 16 != 0) {
      /* auxdata not 128-bit aligned; load alpha[k] into upper part of
         aux2 and then jump to second loop section; changing loop stop
         accordingly. The nk > 2 check protects against doing the jump
         if there's 0 iterations. */
      aux2 = (m128d){ 0.0, auxdata[3 * k] };
      if (nk % 2 == 0) loop_stop--;
      goto SECOND_ITERATION;
    }
    while (k < loop_stop) {
      /* The recurrence relation we compute is, for each x_i,

         P_{k} = (x^2 + [-d_{k-1}]) * [1/c_{k-1}] P_{k-1} + [-c_{k-2}/c_{k-1}] P_{k-2}

         which we write

         P_k = (x^2 + alpha) * beta * P_{k-2} + gamma * P_{k-2}

         The terms in []-brackets are precomputed and available packed
         in auxdata; they must be unpacked into registers. Storing
         c_{k-2}/c_{k-1} seperately removes one dependency in the chain
         to hopefully improve pipelining.

         Data packing: To save memory, and in at least one benchmark 2% running
         time, the data is stored in memory as [(alpha beta) (gamma alpha) (beta gamma) ...].
         That is, we unroll 2 iterations of the loop and load the auxiliary
         data in different ways each time.

         NOTE: I tried to write the logic using an extra two-iteration
         loop, but gcc (v4.4.5) was not able to see through it. Templating
         should be used instead.

         NOTE: This is better compiled WITHOUT -funroll-loops (or at
         least don't assume it doesn't make things
         worse). Profile-guided optimization made things worse as
         well.
       */
      /* Unpack alpha, beta, gamma from aux1 and aux2, and do a loop step. */
      aux1 = _mm_load_pd(auxdata + 3 * k);
      aux2 = _mm_load_pd(auxdata + 3 * k + 2);
      alpha = _mm_unpacklo_pd(aux1, aux1);
      beta = _mm_unpackhi_pd(aux1, aux1);
      gamma = _mm_unpacklo_pd(aux2, aux2);
      {{loop_body(xchunksize)}}
      ++k;

    SECOND_ITERATION:
      /* Unpack alpha, beta, gamma from aux2 and aux3, and do a loop step. */
      aux3 = _mm_load_pd(auxdata + 3 * (k - 1) + 4);
      alpha = _mm_unpackhi_pd(aux2, aux2);
      beta = _mm_unpacklo_pd(aux3, aux3);
      gamma = _mm_unpackhi_pd(aux3, aux3);
      {{loop_body(xchunksize)}}
      ++k;
    }
    if (k != nk) {
      /* Loop peel for the single odd k. */
      aux1 = _mm_load_pd(auxdata + 3 * k);
      alpha = _mm_unpacklo_pd(aux1, aux1);
      beta = _mm_unpackhi_pd(aux1, aux1);
      gamma = (m128d){ auxdata[3 * k + 2], auxdata[3 * k + 2] };
      {{loop_body(xchunksize)}}
      ++k;
    }

    /* Finally, store the computed y_ij's. */
    {{if single}}
    _mm_store_pd(y + i * 2, y_ij[0]);
    {{else}}
    /* Must transpose them */
    m128d ycol_i[s], ycol_ip[s];
    for (s = 0; s != {{nregs}}; ++s) {
      ycol_i[s] = _mm_shuffle_pd(y_ij[s], y_ijp[s], _MM_SHUFFLE2(0, 0));
      ycol_ip[s] = _mm_shuffle_pd(y_ij[s], y_ijp[s], _MM_SHUFFLE2(1, 1));
      _mm_store_pd(y + (i + 2 * s) * 2, ycol_i[s]);
      _mm_store_pd(y + (i + 2 * s + 1) * 2, ycol_ip[s]);
    }
    {{endif}}
  }
}
{{endfor}}


{{for xchunksize in [1, 2, 4]}}
{{py:
single = xchunksize == 1
nregs = 1 if single else xchunksize // 2
}}
#define X_CHUNK_SIZE {{xchunksize}}
#define K_CHUNK_SIZE LEGENDRE_TRANSFORM_WORK_SIZE / (X_CHUNK_SIZE * sizeof(double))
#define NREGS X_CHUNK_SIZE / 2
#define NJ 4

static void legendre_matmul_chunk{{xchunksize}}(size_t k_start, size_t k_stop, size_t nvecs,
                                                double *a, double *y_acc, double *P_block) {
  size_t i, j, s, k, j_chunk_start;
  double *Pptr;

  /* Accumulator for vec j, x-strip i is y_ji[i * NREGS + j] */
  m128d y_ji[NJ * NREGS];

  assert(NJ % 2 == 0);
  assert(nvecs % NJ == 0); /* TODO ! */
  for (j_chunk_start = 0; j_chunk_start != nvecs; j_chunk_start += NJ) {
    /* Load the accumulators from y_acc. While we accumulate we store
       the accumulators in a convenient transposed order (also across calls
       to this function). */
    for (s = 0; s != NJ * NREGS; ++s) {
      y_ji[s] = _mm_load_pd(y_acc + 2 * s);
    }

    Pptr = P_block;
    for (k = k_start; k != k_stop; ++k) {
      m128d a_ji[NJ];
      m128d Pval;
      /* Load and shuffle a */
      for (s = 0; s != NJ / 2; ++s) {
        a_ji[2 * s] = _mm_load_pd(a + k * nvecs + j_chunk_start + 2 * s);
        a_ji[2 * s + 1] = _mm_unpackhi_pd(a_ji[2 * s], a_ji[2 * s]);
        a_ji[2 * s] = _mm_unpacklo_pd(a_ji[2 * s], a_ji[2 * s]);
      }
      printf("==\n");
      /* Muladds */
      for (i = 0; i != X_CHUNK_SIZE; i += 2) {
        Pval = _mm_load_pd(Pptr);
        Pptr += 2;
        for (j = 0; j != NJ; ++j) {
          y_ji[i * NREGS + j] = MULADD(y_ji[i * NREGS + j], Pval, a_ji[j]);
        }
      }
    }

    /* Store accumulators for the next iteration or the transpose */
    for (s = 0; s != NJ * NREGS; ++s) {
      _mm_store_pd(y_acc + 2 * s, y_ji[s]);
    }
  }

}

static void legendre_transform_chunk{{xchunksize}}(size_t ix_start, size_t ix_stop,
                                                   size_t nk,
                                                   size_t nvecs,
                                                   double *a,
                                                   double *y,
                                                   double *x_squared, 
                                                   double *auxdata,
                                                   double *P0, double *P1,
                                                   char *work) {
  size_t i, k_chunk_start, k_chunk_stop, s, k;
  double *P_block = (double *)work;
  double *P_block_last_two_rows = P_block
    + (LEGENDRE_TRANSFORM_WORK_SIZE / sizeof(double))
    - 2 * X_CHUNK_SIZE;

  auxdata -= 6; /* auxdata not passed for k=0, k=1 */

  assert((ix_stop - ix_start) % X_CHUNK_SIZE == 0);
  assert(K_CHUNK_SIZE >= 2);
  /* Loop over x-strips */
  for (i = ix_start; i != ix_stop; i += X_CHUNK_SIZE) {
    double *y_chunk = y + i * nvecs;

    /* Load x-values -- by keeping a few values on the stack, the x_squared page
       can mostly be forgotten about. */
    m128d x_squared_i[NREGS];
    for (s = 0; s != NREGS; ++s) {
      x_squared_i[s] = _mm_load_pd(x_squared + i + 2 * s);
    }
    /* Deal with P0 and P1. We copy into contiguous buffer, zero y, and call matmul.
       We use the last two rows of P_block as our buffer so that the values are
       ready as initialization values to the strip processing below. */
    double *P = P_block_last_two_rows;
    for (s = 0; s != NREGS; ++s) {
      _mm_store_pd(P, _mm_load_pd(P0 + i + 2 * s));
      P += 2;
    }
    for (s = 0; s != NREGS; ++s) {
      _mm_store_pd(P, _mm_load_pd(P1 + i + 2 * s));
      P += 2;
    }
    for (s = 0; s != NREGS * nvecs; ++s) {
      _mm_store_pd(y_chunk + 2 * s, _mm_setzero_pd());
    }
    legendre_matmul_chunk{{xchunksize}}(0, 2, nvecs, a,
                                        y_chunk, P_block_last_two_rows);

    /* Loop over chunks in K, so that we stay within LEGENDRE_TRANSFORM_WORK_SIZE and
       don't spill P_block out of P1. */
    for (k_chunk_start = 2; k_chunk_start < nk; k_chunk_start += K_CHUNK_SIZE) {
      k_chunk_stop = imin(nk, k_chunk_start + K_CHUNK_SIZE);

      /****
       * Phase 1: Generate P_lm
       *****/
      m128d P_ki[NREGS], Pp_ki[NREGS], Ppp_ki[NREGS];

      /* Load the last two rows of the previous block. */
      P = P_block_last_two_rows;
      for (s = 0; s != NREGS; ++s) {
        Ppp_ki[s] = _mm_load_pd(P);
        P += 2;
      }
      for (s = 0; s != NREGS; ++s) {
        Pp_ki[s] = _mm_load_pd(P);
        P += 2;
      }

      P = P_block;
      assert((intptr_t)auxdata % 2 == 0); /* TODO! */
      assert((k_chunk_stop - k_chunk_start) % 2 == 0); /* TODO! */
      for (k = k_chunk_start; k != k_chunk_stop; ++k) {
        /* Use three-term recurrence formula */
        m128d w[NREGS];
        m128d alpha, beta, gamma, aux1, aux2, aux3;

        {{def loop_body}}
        for (s = 0; s != NREGS; ++s) {
          w[s] = x_squared_i[s];
          w[s] = _mm_add_pd(w[s], alpha);
          w[s] = _mm_mul_pd(w[s], beta);
          w[s] = _mm_mul_pd(w[s], Pp_ki[s]);
        
          P_ki[s] = _mm_mul_pd(Ppp_ki[s], gamma);
          P_ki[s] = _mm_add_pd(P_ki[s], w[s]);
      
          Ppp_ki[s] = Pp_ki[s];
          Pp_ki[s] = P_ki[s];
          _mm_store_pd(P, P_ki[s]);
          P += 2;
        }
        {{enddef}}
        
        /* Unpack alpha, beta, gamma from aux1 and aux2, and do a loop step. */
        aux1 = _mm_load_pd(auxdata + 3 * k);
        aux2 = _mm_load_pd(auxdata + 3 * k + 2);

        alpha = _mm_unpacklo_pd(aux1, aux1);
        beta = _mm_unpackhi_pd(aux1, aux1);
        gamma = _mm_unpacklo_pd(aux2, aux2);
        {{loop_body}}
        ++k;

        /* Unpack alpha, beta, gamma from aux2 and aux3, and do a loop step. */
        aux3 = _mm_load_pd(auxdata + 3 * (k - 1) + 4);
        alpha = _mm_unpackhi_pd(aux2, aux2);
        beta = _mm_unpacklo_pd(aux3, aux3);
        gamma = _mm_unpackhi_pd(aux3, aux3);

        {{loop_body}}

        printf("k=%d\n",k);
        printreg(P_ki[0]);
      }

      /****
       * Phase 2: Matrix multiplication
       *****/
      legendre_matmul_chunk{{xchunksize}}(k_chunk_start, k_chunk_stop, nvecs, a,
                                          y_chunk, P_block);
    }

    /* After the entire strip is processed, we transpose y */
    /*  i * NREGS + j
    for (s = 0; s != NJ * NREGS; ++s) {
      _mm_store_pd(y_acc + 2 * s, y_ji[s]);
    }
    */
    /*    assert(NREGS == 2 && NJ == 4);
    for (j = 0; j != nvecs; j += NJ) {
      m128d y_t[NREGS * NJ];
      for (s = 0; s != NREGS * NJ; ++s) {
        y_t[s] = _mm_load_pd(y_chunk + j * X_CHUNK_SIZE + 2 * s);
      }
      yval[0] = 
      
      }*/
    /* TODO! */
  }
}
#undef X_CHUNK_SIZE
#undef K_CHUNK_SIZE
#undef NREGS
#undef NJ
{{endfor}}

/*
Alignment requirements:

auxdata need only be 64-bit aligned, the other arrays should be 128-bit aligned
*/
void fastsht_associated_legendre_transform_sse(size_t nx, size_t nk,
                                               size_t nvecs,
                                               double *a,
                                               double *y,
                                               double *x_squared, 
                                               double *auxdata,
                                               double *P, double *Pp1,
                                               char *work) {
  /* Function body */

  assert(nk >= 2);
  assert((size_t)a % 16 == 0);
  assert((size_t)y % 16 == 0);
  assert((size_t)P % 16 == 0);
  assert((size_t)Pp1 % 16 == 0);
  size_t i, n;
  i = 0;

{{def chunkdispatch(nvecs, xchunksizes)}}
  {{for xchunksize in xchunksizes}}
  n = nx - nx % {{xchunksize}};
  if (i != n) {
    legendre_transform_chunk{{xchunksize}}{{'_nvec%d' % nvecs if nvecs is not None else ''}}
    (i, n, nk, {{'nvecs, ' if nvecs is None else ''}}a, y, x_squared, auxdata, P, Pp1
    {{', work' if nvecs is None else ''}});
  }
  i = n;
  {{endfor}}
{{enddef}}

  if (nvecs == 2) {
    {{chunkdispatch(2, [6, 2, 1])}}
  } else if (nvecs % 2 == 0) {
    {{chunkdispatch(None, [4, 2, 1])}}
  } else {
    check(0, "nvecs not divisble by 2");
  }
}

void fastsht_associated_legendre_transform(size_t nx, size_t nk,
                                           size_t nvecs,
                                           double *a,
                                           double *y,
                                           double *x_squared, 
                                           double *auxdata,
                                           double *P, double *Pp1) {
  size_t i, k, j;
  double Pval, Pval_prev, Pval_prevprev;

  assert(nk >= 2);
  for (i = 0; i != nx; ++i) {
    /* First get away with the precomputed values. This also zeros the output buffer. */
    k = 0;
    Pval_prevprev = P[i];
    Pval_prev = Pp1[i];
    for (j = 0; j != nvecs; ++j) {
      y[i * nvecs + j] = Pval_prevprev * a[k * nvecs + j];
    }
    ++k;
    for (j = 0; j != nvecs; ++j) {
      y[i * nvecs + j] += Pval_prev * a[k * nvecs + j];
    }
    ++k;
    for (; k < nk; ++k) {
      double alpha = auxdata[3 * (k - 2) + 0];
      double beta = auxdata[3 * (k - 2) + 1];
      double gamma = auxdata[3 * (k - 2) + 2];
      Pval = (x_squared[i] + alpha) * beta * Pval_prev + gamma * Pval_prevprev;
      Pval_prevprev = Pval_prev;
      Pval_prev = Pval;
      for (j = 0; j != nvecs; ++j) {
        y[i * nvecs + j] += Pval * a[k * nvecs + j];        
      }
    }
  }
}

/*
  Compute auxiliary data for the associated Legendre transform. The size
  of the output 'auxdata' buffer should be at least 3 * (nk - 2).
  The first chunk of auxiliary data is for computing P_{lmin + 4}^m.
*/
void fastsht_associated_legendre_transform_auxdata(size_t m, size_t lmin, size_t nk,
                                                   double *auxdata) {
  size_t k, l;
  double c, cp, cpp, d, dp, x, y;
  for (k = 0, l = lmin; k != nk; ++k, l += 2) {
    /* Compute c */
    x = (l - m + 1) * (l - m + 2) * (l + m + 1) * (l + m + 2);
    y = (2 * l + 1) * (2 * l + 3) * (2 * l + 3) * (2 * l + 5);
    c = sqrt(x / y);
    /* Compute d */
    x = 2 * l * (l + 1) - 2 * m * m - 1;
    y = (2 * l - 1) * (2 * l + 3);
    d = x / y;

    if (k >= 2) {
      /* Compute and store auxiliary quantities */
      auxdata[3 * (k - 2)] = -dp; /* alpha */
      auxdata[3 * (k - 2) + 1] = 1 / cp; /* beta */
      auxdata[3 * (k - 2) + 2] = -cpp / cp; /* gamma */
    }
    dp = d;
    cpp = cp;
    cp = c;
  }
}
