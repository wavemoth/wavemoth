#undef NDEBUG
#include <assert.h>
#include <xmmintrin.h>
#include <emmintrin.h>
#include <stdlib.h>
#include <stdio.h>
#include <math.h>
#include <stdint.h>

#include "fastsht_error.h"
#include "legendre_transform.h"

#ifndef INLINE
# if __STDC_VERSION__ >= 199901L
#  define INLINE inline
# else
#  define INLINE
# endif
#endif

typedef __m128d m128d;
typedef __m128 m128;

static void print_array(char *msg, double* arr, size_t len) {
  size_t i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%02.2f ", arr[i]);
  }
  printf("\n");
}

static void _printreg(char *msg, m128d r) {
  double *pd = (double*)&r;
  printf("%s = [%f %f]\n", msg, pd[0], pd[1]);
}

static INLINE size_t imin(size_t a, size_t b) {
  return (a < b) ? a : b;
}

#define printreg(x) _printreg(#x, x)

#define MULADD(z, a, b) _mm_add_pd(z, _mm_mul_pd(a, b))

static INLINE m128d load_single(double *x) {
  return (m128d){ x[0], 0.0 };
}

static INLINE m128d load_single_dup(double *x) {
  return (m128d){ x[0], x[0] };
}


{{for xchunksize in [1, 2, 6]}}
{{py:
single = xchunksize == 1
nregs = 1 if single else xchunksize // 2
}}
static void legendre_transform_chunk{{xchunksize}}_nvec2(size_t ix_start,
                                                         size_t ix_stop,
                                                         size_t nk,
                                                         double *a,
                                                         double *y,
                                                         double *x_squared,
                                                         double *auxdata,
                                                         double *P,
                                                         double *Pp1) {
  size_t i, k, s;
  assert((ix_stop - ix_start) % {{xchunksize}} == 0);

  /* We compute:

        y_{k,j} = sum_k  P_{k, i} a_{k,j}

     Overall strategy: Compute P with three-term recurrence relation
     in k and multiply results with a as we go. We compute with xchunksize=6
     columns of P at the time, which is what we can fit in the 16
     registers available without spilling. The shuffling&loading of
     'a' and auxiliary data (alpha, beta, gamma) is amortized over
     these 6 columns. 'x_squared' is streamed in again and again (once
     per row of P) to conserve registers -- letting NS=4 or spilling
     registers into stack were both significantly slower).

     The output 'y' sits in 6 SSE accumulation registers (12 values
     at the time) and is transposed and stored only after each
     loop over k.
    
  */

  auxdata -= 6; /* auxdata not passed for k=0, k=1 */

  /* Process strips (xchunksize={{xchunksize}}) */
  for (i = ix_start; i != ix_stop; i += {{xchunksize}}) {
    /* In comments and variable names we will assume that k starts on
       0 to keep things brief; however, this is a dynamic quantity to
       support ignoring near-zero parts of P. */
    k = 0;
    /* We loop over k in the inner-most loop and fully compute y_ij
       before storing it.

       If xchunksize == 1, computations on P_ki is duplicated on both
       register entries, and SSE is only used for the 'matrix multiplication'
       step:

         P_ki = [P_{k,i} P_{k,i}]  (same for Pp_ki, Ppp_ki)
         a_kj = [ a_{k,j}  a_{k,j+1} ]
         y_ij = [y_{i,j}  y_{i,j+1}]

       Otherwise, we process two strips at the time in P_ki. The
       accumulation registers y are kept in transposed order, and then
       transposed in the end:

         P_ki =   [P_{k,i} P_{k,i+1}] (same for Pp_ki, Ppp_ki)

         a_kj  = [ a_{k,j}  a_{k,j} ]
         a_kjp = [ a_{k,j+1}  a_{k,j+1} ]
    
         y_ij  = [y_{i,j}   y_{i+1, j}  ]
         y_ijp = [y_{i,j+1} y_{i+1,j+1} ]

    */
    m128d y_ij[{{nregs}}], a_kj;
    m128d P_ki[{{nregs}}], Pp_ki[{{nregs}}], Ppp_ki[{{nregs}}];
    {{if not single}}
    m128d y_ijp[{{nregs}}], a_kjp;
    {{endif}}


    /* Template for loading a_kj */
    {{def load_a}}
    {
      a_kj = _mm_load_pd(a + k * 2);
      {{if not single}}
        a_kjp = _mm_unpackhi_pd(a_kj, a_kj);
        a_kj = _mm_unpacklo_pd(a_kj, a_kj);
      {{endif}}
    }
    {{enddef}}

    /* Template for the core loop body

       Uses recurrence relation and does matmul for a strip of xchunksize
       elements.

       INPUT (C variables assumed to be present):
     
       alpha, beta, gamma:
         Auxiliary information, values duplicated across
         register elements.

       k, i:
         Counters set to row and column of P matrix. P[k, i:i + xchunksize]
         is processed.

       x_squared is streamed in as we go in order to conserve
       registers, so that we can let xchunksize == 6 instead of 4. The extra
       loads we spend here are worth it so that the loads&shuffles of
       alpha, beta, gamma, and a can be reused over 3 stripes instead
       of 2.
         
       This had a noticeable impact on performance, from around 75%
       of peak GFLOP to 85%.
    */
    {{def loop_body(xchunksize)}}
    {
      m128d w[{{nregs}}];
      for (s = 0; s != {{nregs}}; ++s) {
        {{if single}}
          ((double*)&w[s])[0] = x_squared[i + 2 * s];
          w[s] = _mm_unpacklo_pd(w[s], w[s]);
        {{else}}
          w[s] = _mm_load_pd(x_squared + i + 2 * s);
        {{endif}}
        w[s] = _mm_add_pd(w[s], alpha);
        w[s] = _mm_mul_pd(w[s], beta);
        w[s] = _mm_mul_pd(w[s], Pp_ki[s]);
      
        P_ki[s] = _mm_mul_pd(Ppp_ki[s], gamma);
        P_ki[s] = _mm_add_pd(P_ki[s], w[s]);
      
        Ppp_ki[s] = Pp_ki[s];
        Pp_ki[s] = P_ki[s];
      }
      {{load_a}}
      for (s = 0; s != {{nregs}}; ++s) {
        y_ij[s] = MULADD(y_ij[s], P_ki[s], a_kj);
        {{if not single}}
          y_ijp[s] = MULADD(y_ijp[s], P_ki[s], a_kjp);
        {{endif}}
      }
    }
    {{enddef}}


    /* Move on to computation.
       First two values of k needs special treatment as they are
       already computed (starting values). For the first k we
       initialize y_ij, and after that we accumulate in y_ij.
    */

    {{load_a}}
    {{if single}}
      Ppp_ki[0] = (m128d){ P[i], P[i] };
      y_ij[0] = _mm_mul_pd(Ppp_ki[0], a_kj);
    {{else}}
      for (s = 0; s != {{nregs}}; ++s) {
        Ppp_ki[s] = _mm_load_pd(P + i + 2 * s);
        y_ij[s] = _mm_mul_pd(Ppp_ki[s], a_kj);
        y_ijp[s] = _mm_mul_pd(Ppp_ki[s], a_kjp);
      }
    {{endif}}

    ++k;
    {{load_a}}

    {{if single}}
      Pp_ki[0] = (m128d){ Pp1[i], Pp1[i] };
      y_ij[0] = MULADD(y_ij[0], Pp_ki[0], a_kj);
    {{else}}
      for (s = 0; s != {{nregs}}; ++s) {
        Pp_ki[s] = _mm_load_pd(Pp1 + i + 2 * s);
        y_ij[s] = MULADD(y_ij[s], Pp_ki[s], a_kj);
        y_ijp[s] = MULADD(y_ijp[s], Pp_ki[s], a_kjp);
      }
    {{endif}}
    ++k;

    m128d aux1, aux2, aux3, alpha, beta, gamma;
    size_t loop_stop = nk - nk % 2;
    if (nk > 2 && (intptr_t)auxdata % 16 != 0) {
      /* auxdata not 128-bit aligned; load alpha[k] into upper part of
         aux2 and then jump to second loop section; changing loop stop
         accordingly. The nk > 2 check protects against doing the jump
         if there's 0 iterations. */
      aux2 = (m128d){ 0.0, auxdata[3 * k] };
      if (nk % 2 == 0) loop_stop--;
      goto SECOND_ITERATION;
    }
    while (k < loop_stop) {
      /* The recurrence relation we compute is, for each x_i,

         P_{k} = (x^2 + [-d_{k-1}]) * [1/c_{k-1}] P_{k-1} + [-c_{k-2}/c_{k-1}] P_{k-2}

         which we write

         P_k = (x^2 + alpha) * beta * P_{k-2} + gamma * P_{k-2}

         The terms in []-brackets are precomputed and available packed
         in auxdata; they must be unpacked into registers. Storing
         c_{k-2}/c_{k-1} seperately removes one dependency in the chain
         to hopefully improve pipelining.

         Data packing: To save memory, and in at least one benchmark 2% running
         time, the data is stored in memory as [(alpha beta) (gamma alpha) (beta gamma) ...].
         That is, we unroll 2 iterations of the loop and load the auxiliary
         data in different ways each time.

         NOTE: I tried to write the logic using an extra two-iteration
         loop, but gcc (v4.4.5) was not able to see through it. Templating
         should be used instead.

         NOTE: This is better compiled WITHOUT -funroll-loops (or at
         least don't assume it doesn't make things
         worse). Profile-guided optimization made things worse as
         well.
       */
      /* Unpack alpha, beta, gamma from aux1 and aux2, and do a loop step. */
      aux1 = _mm_load_pd(auxdata + 3 * k);
      aux2 = _mm_load_pd(auxdata + 3 * k + 2);
      alpha = _mm_unpacklo_pd(aux1, aux1);
      beta = _mm_unpackhi_pd(aux1, aux1);
      gamma = _mm_unpacklo_pd(aux2, aux2);
      {{loop_body(xchunksize)}}
      ++k;

    SECOND_ITERATION:
      /* Unpack alpha, beta, gamma from aux2 and aux3, and do a loop step. */
      aux3 = _mm_load_pd(auxdata + 3 * (k - 1) + 4);
      alpha = _mm_unpackhi_pd(aux2, aux2);
      beta = _mm_unpacklo_pd(aux3, aux3);
      gamma = _mm_unpackhi_pd(aux3, aux3);
      {{loop_body(xchunksize)}}
      ++k;
    }
    if (k != nk) {
      /* Loop peel for the single odd k. */
      aux1 = _mm_load_pd(auxdata + 3 * k);
      alpha = _mm_unpacklo_pd(aux1, aux1);
      beta = _mm_unpackhi_pd(aux1, aux1);
      gamma = (m128d){ auxdata[3 * k + 2], auxdata[3 * k + 2] };
      {{loop_body(xchunksize)}}
      ++k;
    }

    /* Finally, store the computed y_ij's. */
    {{if single}}
    _mm_store_pd(y + i * 2, y_ij[0]);
    {{else}}
    /* Must transpose them */
    m128d ycol_i[s], ycol_ip[s];
    for (s = 0; s != {{nregs}}; ++s) {
      ycol_i[s] = _mm_shuffle_pd(y_ij[s], y_ijp[s], _MM_SHUFFLE2(0, 0));
      ycol_ip[s] = _mm_shuffle_pd(y_ij[s], y_ijp[s], _MM_SHUFFLE2(1, 1));
      _mm_store_pd(y + (i + 2 * s) * 2, ycol_i[s]);
      _mm_store_pd(y + (i + 2 * s + 1) * 2, ycol_ip[s]);
    }
    {{endif}}
  }
}
{{endfor}}



{{py:
xchunksize_manyvec_list = [4, 2, 1]
}}

/* A is repacked so as to match the max xchunksize. */
#define MAX_X_CHUNK_SIZE {{max(xchunksize_manyvec_list)}}
#define K_CHUNK_SIZE LEGENDRE_TRANSFORM_WORK_SIZE / (MAX_X_CHUNK_SIZE * sizeof(double) * 2)
#define NJ 6

static void pack_A(double *A, double *buf, size_t nvecs, size_t nk) {
  size_t k, j_start, k_start, k_stop, s;
  double *pbuf, *A_chunk;

  /* Can we always process 2 chunks per K_CHUNK? Does it matter? */

  /* k is chunked as [2, K_CHUNK_SIZE, K_CHUNK_SIZE, ...], since
     the first two rows gets special treatment.*/
  for (k_start = 0; k_start < nk; k_start += K_CHUNK_SIZE) {
    k_stop = imin(nk, k_start + K_CHUNK_SIZE);
    pbuf = buf;
    /* First transpose into temporary buffer buf */
    for (j_start = 0; j_start < nvecs; j_start += NJ) {
      for (k = k_start; k != k_stop; ++k) {
        for (s = 0; s != NJ / 2; ++s) {
          m128d a = _mm_load_pd(A + k * nvecs + j_start + 2 * s);
          _mm_store_pd(pbuf, a);
          pbuf += 2;
        }
      }
    }
    /* Copy back */
    pbuf = buf;
    A_chunk = A + k_start * nvecs;
    for (s = 0; s != (k_stop - k_start) * nvecs; s += 2) {
      _mm_store_pd(A_chunk + s, _mm_load_pd(pbuf + s));
    }
  }
}


{{for xchunksize in xchunksize_manyvec_list}}
{{py:
single = xchunksize == 1
nregs = 1 if single else xchunksize // 2
}}
#define X_CHUNK_SIZE {{xchunksize}}
#define NREGS {{nregs}}
static void legendre_matmul_chunk{{xchunksize}}(size_t nk, size_t nvecs,
                                                double *A, double *y_acc, double *P_block) {
  size_t i, s, k, j_chunk_start;
  double *pP, *pA;

  /* Accumulator for vec j, x-strip i is y_ji[i * (NJ / 2) + j]. Each y_ji
     contains two j-values; y_ij = [ y_{j,i}   y_{j+1, i} ]. */
  m128d y_ji[(NJ / 2) * X_CHUNK_SIZE];
  
  assert(NJ % 2 == 0);
  assert(nvecs % NJ == 0); /* TODO ! */
  for (j_chunk_start = 0; j_chunk_start != nvecs; j_chunk_start += NJ) {
    /* Initialize accumulators to 0 */
    for (s = 0; s != (NJ / 2) * X_CHUNK_SIZE; ++s) {
      y_ji[s] = _mm_setzero_pd();
    }

    pP = P_block;
    pA = A + j_chunk_start * nk;
    for (k = 0; k != nk; ++k) {
      m128d a_ji[NJ / 2];
      m128d Pval;
      /* Load a */
      for (s = 0; s != NJ / 2; ++s) {
        a_ji[s] = _mm_load_pd(pA);
        pA += 2;
      }
      /* Muladds */
      for (i = 0; i != X_CHUNK_SIZE; ++i) {
        Pval = _mm_load_pd(pP);
        pP += 2;
        for (s = 0; s != NJ / 2; ++s) {
          y_ji[i * (NJ / 2) + s] = MULADD(y_ji[i * (NJ / 2) + s], Pval, a_ji[s]);
        }
      }
    }

    /* Accumulate in vector */
    for (i = 0; i != X_CHUNK_SIZE; ++i) {
      for (s = 0; s != NJ / 2; ++s) {
        m128d acc = _mm_load_pd(y_acc + i * nvecs + j_chunk_start + 2 * s);
        acc = _mm_add_pd(acc, y_ji[i * (NJ / 2) + s]);
        _mm_store_pd(y_acc + i * nvecs + j_chunk_start + 2 * s, acc);
      }
    }
  }
}

static void legendre_transform_chunk{{xchunksize}}(size_t ix_start, size_t ix_stop,
                                                   size_t nk,
                                                   size_t nvecs,
                                                   double *A,
                                                   double *Y,
                                                   double *x_squared, 
                                                   double *auxdata,
                                                   double *P0, double *P1,
                                                   char *work) {
  size_t i, k_chunk_start, k_chunk_stop, s, k;
  double *P_block = (double *)work;
  double *P_block_last_two_rows = P_block + 2 * (K_CHUNK_SIZE - 2) * X_CHUNK_SIZE;
  double *A_chunk;
  auxdata -= 6; /* auxdata not passed for k=0, k=1 */

  check(MAX_X_CHUNK_SIZE >= X_CHUNK_SIZE, "Adjust MAX_X_CHUNK_SIZE");
  assert((ix_stop - ix_start) % X_CHUNK_SIZE == 0);
  assert(K_CHUNK_SIZE >= 2);

  /* Loop over x-strips */
  for (i = ix_start; i != ix_stop; i += X_CHUNK_SIZE) {
    m128d P_ki[NREGS], Pp_ki[NREGS], Ppp_ki[NREGS];
    double *Y_chunk = Y + i * nvecs;

    /* Load x-values. This spills over to stack for each call to matmul;
       so one more store, but can keep x_squared unpaged for longer.
       Could play around with this. */
    m128d x_squared_i[NREGS];
    {{if single}}
    x_squared_i[0] = load_single_dup(x_squared + i);
    {{else}}
    for (s = 0; s != NREGS; ++s) {
      x_squared_i[s] = _mm_load_pd(x_squared + i + 2 * s);
    }
    {{endif}}

    /* Initialize Y to zero. */
    for (s = 0; s != X_CHUNK_SIZE * nvecs / 2; ++s) {
      _mm_store_pd(Y_chunk + 2 * s, _mm_setzero_pd());
    }

    /* Deal with P0 and P1. Simply copy to the end of P_block and
       do a matmul. */
    double *pP = P_block_last_two_rows;
    {{for arr in ['P0', 'P1']}}
    for (s = 0; s != NREGS; ++s) {
      m128d lo, hi;
      {{if single}}
      lo = load_single_dup({{arr}} + i + 2 * s);
      _mm_store_pd(pP, lo);
      pP += 2;
      {{else}}
      lo = _mm_load_pd({{arr}} + i + 2 * s);
      hi = _mm_unpackhi_pd(lo, lo);
      lo = _mm_unpacklo_pd(lo, lo);
      _mm_store_pd(pP, lo);
      _mm_store_pd(pP + 2, hi);
      pP += 4;
      {{endif}}
    }
    {{endfor}}

    legendre_matmul_chunk{{xchunksize}}(2, nvecs,
                                        A, Y_chunk,
                                        P_block_last_two_rows);


    /* Loop over chunks in K, so that we stay within LEGENDRE_TRANSFORM_WORK_SIZE and
       don't spill P_block out of L1 cache. */
    for (k_chunk_start = 2; k_chunk_start < nk; k_chunk_start += K_CHUNK_SIZE) {
      k_chunk_stop = imin(nk, k_chunk_start + K_CHUNK_SIZE);
      /****
       * Phase 1: Generate P_lm
       *****/
      A_chunk = A + k_chunk_start * nvecs;

      /* Load the last two rows of the previous block into registers. */
      pP = P_block_last_two_rows;
      {{if single}}
      Ppp_ki[0] = _mm_load_pd(pP);
      Pp_ki[0] = _mm_load_pd(pP + 2);
      {{else}}
      {{for regarr in ['Ppp_ki', 'Pp_ki']}}
      for (s = 0; s != NREGS; ++s) {
        m128d lo, hi;
        lo = _mm_load_pd(pP);
        hi = _mm_load_pd(pP + 2);
        pP += 4;
        {{regarr}}[s] = _mm_unpacklo_pd(lo, hi);
      }
      {{endfor}}
      {{endif}}

      pP = P_block;
      assert((intptr_t)auxdata % 2 == 0); /* TODO! */
      assert((k_chunk_stop - k_chunk_start) % 2 == 0); /* TODO! */
      for (k = k_chunk_start; k != k_chunk_stop; ++k) {
        /* Use three-term recurrence formula */
        m128d w[NREGS];
        m128d alpha, beta, gamma, aux1, aux2, aux3, lo, hi;

        {{def loop_body}}
        for (s = 0; s != NREGS; ++s) {
          w[s] = x_squared_i[s];
          w[s] = _mm_add_pd(w[s], alpha);
          w[s] = _mm_mul_pd(w[s], beta);
          w[s] = _mm_mul_pd(w[s], Pp_ki[s]);
        
          P_ki[s] = _mm_mul_pd(Ppp_ki[s], gamma);
          P_ki[s] = _mm_add_pd(P_ki[s], w[s]);
      
          Ppp_ki[s] = Pp_ki[s];
          Pp_ki[s] = P_ki[s];

          {{if single}}
          _mm_store_pd(pP, P_ki[s]);
          pP += 2;
          {{else}}
          lo = _mm_unpacklo_pd(P_ki[s], P_ki[s]);
          hi = _mm_unpackhi_pd(P_ki[s], P_ki[s]);
          _mm_store_pd(pP, lo);
          _mm_store_pd(pP + 2, hi);
          pP += 4;
          {{endif}}
        }
        {{enddef}}
        
        /* Unpack alpha, beta, gamma from aux1 and aux2, and do a loop step. */
        aux1 = _mm_load_pd(auxdata + 3 * k);
        aux2 = _mm_load_pd(auxdata + 3 * k + 2);

        alpha = _mm_unpacklo_pd(aux1, aux1);
        beta = _mm_unpackhi_pd(aux1, aux1);
        gamma = _mm_unpacklo_pd(aux2, aux2);
        {{loop_body}}
        ++k;

        /* Unpack alpha, beta, gamma from aux2 and aux3, and do a loop step. */
        aux3 = _mm_load_pd(auxdata + 3 * (k - 1) + 4);
        alpha = _mm_unpackhi_pd(aux2, aux2);
        beta = _mm_unpacklo_pd(aux3, aux3);
        gamma = _mm_unpackhi_pd(aux3, aux3);

        {{loop_body}}
      }

      /****
       * Phase 2: Matrix multiplication
       *****/
      legendre_matmul_chunk{{xchunksize}}(k_chunk_stop - k_chunk_start, nvecs,
                                          A + k_chunk_start * nvecs, Y_chunk, P_block);
    }
  }
}
#undef X_CHUNK_SIZE
#undef NREGS
{{endfor}}



size_t fastsht_associated_legendre_transform_sse_query_work(size_t nvecs) {
  size_t P_block_size = LEGENDRE_TRANSFORM_WORK_SIZE;
  size_t a_packed_size = nvecs * K_CHUNK_SIZE * sizeof(double);
  if (nvecs == 2) {
    return 0;
  } else {
    return P_block_size + a_packed_size;
  }
}

/*
Alignment requirements:

auxdata need only be 64-bit aligned, the other arrays should be 128-bit aligned
*/
void fastsht_associated_legendre_transform_sse(size_t nx, size_t nk,
                                               size_t nvecs,
                                               double *a,
                                               double *y,
                                               double *x_squared, 
                                               double *auxdata,
                                               double *P, double *Pp1,
                                               char *work) {
  /* Function body */

  assert(nk >= 2);
  assert((size_t)a % 16 == 0);
  assert((size_t)y % 16 == 0);
  assert((size_t)P % 16 == 0);
  assert((size_t)Pp1 % 16 == 0);
  size_t i, n;
  i = 0;

{{def chunkdispatch(nvecs, xchunksizes)}}
  {{for xchunksize in xchunksizes}}
  n = nx - nx % {{xchunksize}};
  if (i != n) {
    legendre_transform_chunk{{xchunksize}}{{'_nvec%d' % nvecs if nvecs is not None else ''}}
    (i, n, nk, {{'nvecs, ' if nvecs is None else ''}}a, y, x_squared, auxdata, P, Pp1
    {{', work' if nvecs is None else ''}});
  }
  i = n;
  {{endfor}}
{{enddef}}

  if (nvecs == 2) {
    {{chunkdispatch(2, [6, 2, 1])}}
  } else if (nvecs % 2 == 0) {
    pack_A(a, (double*)work, nvecs, 2);
    pack_A(a + 2 * nvecs, (double*)work, nvecs, nk - 2);
    {{chunkdispatch(None, xchunksize_manyvec_list)}}
  } else {
    check(0, "nvecs not divisble by 2");
  }
}

void fastsht_associated_legendre_transform(size_t nx, size_t nk,
                                           size_t nvecs,
                                           double *a,
                                           double *y,
                                           double *x_squared, 
                                           double *auxdata,
                                           double *P, double *Pp1) {
  size_t i, k, j;
  double Pval, Pval_prev, Pval_prevprev;

  assert(nk >= 2);
  for (i = 0; i != nx; ++i) {
    /* First get away with the precomputed values. This also zeros the output buffer. */
    k = 0;
    Pval_prevprev = P[i];
    Pval_prev = Pp1[i];
    for (j = 0; j != nvecs; ++j) {
      y[i * nvecs + j] = Pval_prevprev * a[k * nvecs + j];
    }
    ++k;
    for (j = 0; j != nvecs; ++j) {
      y[i * nvecs + j] += Pval_prev * a[k * nvecs + j];
    }
    ++k;
    for (; k < nk; ++k) {
      double alpha = auxdata[3 * (k - 2) + 0];
      double beta = auxdata[3 * (k - 2) + 1];
      double gamma = auxdata[3 * (k - 2) + 2];
      Pval = (x_squared[i] + alpha) * beta * Pval_prev + gamma * Pval_prevprev;
      Pval_prevprev = Pval_prev;
      Pval_prev = Pval;
      for (j = 0; j != nvecs; ++j) {
        y[i * nvecs + j] += Pval * a[k * nvecs + j];        
      }
    }
  }
}

/*
  Compute auxiliary data for the associated Legendre transform. The size
  of the output 'auxdata' buffer should be at least 3 * (nk - 2).
  The first chunk of auxiliary data is for computing P_{lmin + 4}^m.
*/
void fastsht_associated_legendre_transform_auxdata(size_t m, size_t lmin, size_t nk,
                                                   double *auxdata) {
  size_t k, l;
  double c, cp, cpp, d, dp, x, y;
  for (k = 0, l = lmin; k != nk; ++k, l += 2) {
    /* Compute c */
    x = (l - m + 1) * (l - m + 2) * (l + m + 1) * (l + m + 2);
    y = (2 * l + 1) * (2 * l + 3) * (2 * l + 3) * (2 * l + 5);
    c = sqrt(x / y);
    /* Compute d */
    x = 2 * l * (l + 1) - 2 * m * m - 1;
    y = (2 * l - 1) * (2 * l + 3);
    d = x / y;

    if (k >= 2) {
      /* Compute and store auxiliary quantities */
      auxdata[3 * (k - 2)] = -dp; /* alpha */
      auxdata[3 * (k - 2) + 1] = 1 / cp; /* beta */
      auxdata[3 * (k - 2) + 2] = -cpp / cp; /* gamma */
    }
    dp = d;
    cpp = cp;
    cp = c;
  }
}
