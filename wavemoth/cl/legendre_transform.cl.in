#pragma OPENCL EXTENSION cl_khr_fp64: enable

#define NVECS {{nvecs}}
#define LOCAL_SIZE {{local_size}}
#define APPROXIMATE_AUX 1
#define ithread get_local_id(0)

/*****
 * Utilities
 *****/


/* Multiply four 16-bit ints and produce 64-bit result safely. */
inline unsigned long prod4_16(unsigned short a, unsigned short b,
                              unsigned short c, unsigned short d) {
  return (unsigned long)((unsigned int)a * b) * ((unsigned int)c * d);
}

double get_c_squared_num(unsigned short m, unsigned short l) {
  return prod4_16(l - m + 1, l - m + 2,
                  l + m + 1, l + m + 2);
}

double get_c_squared_den(unsigned short m, unsigned short l) {
  return prod4_16(2 * l + 1, 2 * l + 3,
                  2 * l + 3, 2 * l + 5);
}

double get_c(unsigned short m, unsigned short l) {
  /* A reciprocial square-root of the inverse whould be faster,
     but cost 1 ulp precision, and this step is very amortized. */
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#else
  return sqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#endif
}

double get_c_inv(unsigned short m, unsigned short l) {
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#else
  return sqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#endif
}

double get_d(unsigned short m, unsigned short l) {
  double num = 2 * l * (l + 1) - 2 * m * m - 1;
  double den = (2 * l - 1) * (2 * l + 3);
  return num / den;
}


/*****
 * Main code
 *****/

void compute_recurrence_auxiliaries(int m, int lmin, int nk,
                                    local double *alpha,
                                    local double *beta,
                                    local double *gamma) {
  if (ithread >= nk) return;
  int l = lmin + 2 * ithread;

  alpha[ithread] = -get_d(m, l - 2);

  double cpp = get_c(m, l - 4);
  double cp_inv = get_c_inv(m, l - 2);
  beta[ithread] = cp_inv;
  gamma[ithread] = -cpp * cp_inv;
}



/*****
 * Kernel entry points
 *****/

kernel void transpose_legendre_transform(int m, int lmin, int nk, int nx,
                                         global const double *x_squared,
                                         global const double *Lambda_0,
                                         global const double *Lambda_1,
                                         global const double *q,
                                         global double *out) {
  /* Local scratch */
  local double out_work[NVECS * LOCAL_SIZE]; /* shape=(NVECS, LOCAL_SIZE) */
  local double alpha[LOCAL_SIZE], beta[LOCAL_SIZE], gamma[LOCAL_SIZE];

  int iblock = get_global_id(0) / LOCAL_SIZE;
  int ix = get_local_id(0);
  double Pval, Pval_prev, Pval_prevprev;
  
  /* Shift to our block */
  x_squared += iblock * nx;
  Lambda_0 += iblock * nx;
  Lambda_1 += iblock * nx;
  q += iblock * nx * NVECS;
  out += iblock * nk * NVECS;

  /* Compute auxiliaries */
  compute_recurrence_auxiliaries(m, lmin, nk, alpha, beta, gamma);
  barrier(CLK_LOCAL_MEM_FENCE);

  double xsq_val = x_squared[ix];

  for (int i = 0; i != NVECS * LOCAL_SIZE; ++i) {
    out_work[i] = -1;
  }
  barrier(CLK_LOCAL_MEM_FENCE);
      
  Pval_prevprev = Lambda_0[ix];
  Pval_prev = Lambda_1[ix];
  Pval = Pval_prev;

  int k;
  /* Handle matvec of first two rows */

  /* Matvec row 0  */
  k = 0;
  for (int j = 0; j != NVECS; ++j) {
    out_work[j * LOCAL_SIZE + ithread] = Pval_prevprev * q[j * nx + ix];
  }    
  /* Then a very stupid reduction */
  barrier(CLK_LOCAL_MEM_FENCE);
  if (ithread == 0) {
    for (int j = 0; j != NVECS; ++j) {
      double s = 0;
      for (int t = 0; t != LOCAL_SIZE; ++t) {
        s += out_work[j * LOCAL_SIZE + t];
      }
      out[j * nk + k] = s;
    }
  }
  barrier(CLK_LOCAL_MEM_FENCE);

  k = 1;
  for (int j = 0; j != NVECS; ++j) {
    out_work[j * LOCAL_SIZE + ithread] = Pval_prev * q[j * nx + ix];
  }    
  /* Then a very stupid reduction */
  barrier(CLK_LOCAL_MEM_FENCE);
  if (ithread == 0) {
    for (int j = 0; j != NVECS; ++j) {
      double s = 0;
      for (int t = 0; t != LOCAL_SIZE; ++t) {
        s += out_work[j * LOCAL_SIZE + t];
      }
      out[j * nk + k] = s;
    }
  }
  barrier(CLK_LOCAL_MEM_FENCE);


  /* Recurrence */

  for (int k = 2; k != nk; ++k) {
    /* Do recurrence */
    Pval = (xsq_val + alpha[k]) * beta[k] * Pval_prev + gamma[k] * Pval_prevprev;
    Pval_prevprev = Pval_prev;
    Pval_prev = Pval;

    /* Do matvec: First compute our contribution... */
    for (int j = 0; j != NVECS; ++j) {
      out_work[j * LOCAL_SIZE + ithread] = Pval * q[j * nx + ix];
    }    
    /* Then a very stupid reduction */
    barrier(CLK_LOCAL_MEM_FENCE);
    if (ithread == 0) {
      for (int j = 0; j != NVECS; ++j) {
        double s = 0;
        for (int t = 0; t != LOCAL_SIZE; ++t) {
          s += out_work[j * LOCAL_SIZE + t];
        }
        out[j * nk + k] = s;
      }
    }
    barrier(CLK_LOCAL_MEM_FENCE);
  }

}
