{{py:
def log2(x):
    r = -1
    while x != 0:
        x //= 2
        r += 1
    return r
}}

#pragma OPENCL EXTENSION cl_khr_fp64: enable

#define NVECS {{nvecs}}
#define NTHREADS {{local_size}}
#define APPROXIMATE_AUX 1
#define MAX_NI {{max_ni}}
#define WARP_SIZE {{warp_size}}
#define NWARPS {{local_size // warp_size}}
{{default memory_bank_count = warp_size}}
#define K_CHUNK {{k_chunk}}

/* Cast get_local_id(0) to int to make it easier to printf, as all
   our integers are then 32-bit. */
#define ithread ((int)get_local_id(0))
#define iwarp (ithread / WARP_SIZE)

{{if has_warps}}
#define WARP_BARRIER() 
{{else}}
#define WARP_BARRIER() barrier(CLK_LOCAL_MEM_FENCE)
{{endif}}




/*****
 * Utilities
 *****/
/*
void print_array(constant const char *msg, local double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}

void print_array_glob(constant const char *msg, global double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}
*/

/* Multiply four 16-bit ints and produce 64-bit result safely. */
inline unsigned long prod4_16(unsigned short a, unsigned short b,
                              unsigned short c, unsigned short d) {
  return (unsigned long)((unsigned int)a * b) * ((unsigned int)c * d);
}


/*****
 * Auxiliary value computation
 *****/

double get_c_squared_num(unsigned short m, unsigned short l) {
  return prod4_16(l - m + 1, l - m + 2,
                  l + m + 1, l + m + 2);
}

double get_c_squared_den(unsigned short m, unsigned short l) {
  return prod4_16(2 * l + 1, 2 * l + 3,
                  2 * l + 3, 2 * l + 5);
}

double get_c(unsigned short m, unsigned short l) {
  /* A reciprocial square-root of the inverse whould be faster,
     but cost 1 ulp precision, and this step is very amortized. */
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#else
  return sqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#endif
}

double get_c_inv(unsigned short m, unsigned short l) {
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#else
  return sqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#endif
}

double get_d(unsigned short m, unsigned short l) {
  double num = 2 * l * (l + 1) - 2 * m * m - 1;
  double den = (2 * l - 1) * (2 * l + 3);
  return num / den;
}

void compute_recurrence_auxiliaries(int m, int lmin, int nk,
                                    local double *alpha,
                                    local double *beta,
                                    local double *gamma) {
  if (ithread >= nk) return;
  int l = lmin + 2 * ithread;

  alpha[ithread] = -get_d(m, l - 2);

  double cpp = get_c(m, l - 4);
  double cp_inv = get_c_inv(m, l - 2);
  beta[ithread] = cp_inv;
  gamma[ithread] = -cpp * cp_inv;
}

/*****
 * Main code
 *****/

{{py:
kernel_or_not = [('kernel', '_kernel', 'global'),
                 ('', '', 'local')]
}}

/* Do a dot product of the 1D vector P with NVECS vectors
   stored in q, and simultaneously copy P into local memory
   P_local. */
{{for spec, suffix, scope in kernel_or_not}}
{{spec}} void dot_and_copy{{suffix}}(global const double *P,
                                     global const double *q,
                                     {{scope}} double *P_local,
                                     {{scope}} double *work_sum,
                                     int ni) {
  double s[NVECS];
  for (int j = 0; j != NVECS; ++j) {
    s[j] = 0;
  }
  for (int i = ithread; i < ni; i += NTHREADS) {
    double Pval = P[i];
    P_local[i] = Pval;
    for (int j = 0; j != NVECS; ++j) {
      s[j] += Pval * q[j * ni + i];
    }
  }
  for (int j = 0; j != NVECS; ++j) {
    work_sum[j * NTHREADS + ithread] = s[j];
  }
}
{{endfor}}

/* thread_sums[j, ithread] is summed over the threads in each warp and
   result placed in warp_sums[iwarp, k_offset, j].
   Contents of thread_sums is destructed in the process. */
{{for spec, suffix, scope in kernel_or_not}}
{{spec}} void warp_sum_reduce{{suffix}}(int k_offset,
                                        {{scope}} double *thread_sums,
                                        {{scope}} double *warp_sums) {
    /* TODO: Use parallelism to work on multiple vectors in parallel... */
  WARP_BARRIER();
  {{for p in range(log2(warp_size))}}
  {{py:stride = 2**p}}
  if (ithread % {{2 * stride}} == 0) {
    for (int j = 0; j != NVECS; ++j) {
      {{if 2 * stride == warp_size}}
      {{# Write to warp_sums }}
      warp_sums[iwarp * K_CHUNK * NVECS + k_offset * NVECS + j] = 
        (thread_sums[j * NTHREADS + ithread] +
        thread_sums[j * NTHREADS + ithread + {{stride}}]);
      {{else}}
      {{# Write temporary result to thread_sums}}
      thread_sums[j * NTHREADS + ithread] += thread_sums[j * NTHREADS + ithread + {{stride}}];
    {{endif}}
    }
  }
  WARP_BARRIER();
  {{endfor}}
}
{{endfor}}


/* Sum up a number of output buffers (one per warp) in
   `work_local_sum`, and put the result in `out`.  `work_local_sum`
   is a 3D array indexed by [iwarp, k_offset, j] and dimensions
   (NWARPS, K_CHUNK, NVECS).
*/

{{for spec, suffix, scope in kernel_or_not}}
{{spec}} void inter_warp_sum{{suffix}}(int k_start, int nk,
                                       {{scope}}  double *work_local_sum,
                                       global double *out,
                                       int out_stride) {
  /* Have threads divide evenly over the different vectors, gracefully
     handling different nk. */
  for (int idx = ithread; idx < NVECS * nk; idx += NTHREADS) {
    double s = 0;
    for (int t = 0; t != NWARPS; ++t) {
      s += work_local_sum[t * K_CHUNK * NVECS + idx]; /*idx == k * NVECS + j*/
    }
    int j = idx % NVECS;
    int k = idx / NVECS;
    out[j * out_stride + k_start + k] = s;
  }
}
{{endfor}}

/*****
 * Kernel entry points
 *****/

kernel void transpose_legendre_transform(int m, int lmin, int nk, int ni,
                                         global const double *x_squared,
                                         global const double *Lambda_0,
                                         global const double *Lambda_1,
                                         global const double *q,
                                         global double *out) {
  /* Local scratch. We process NTHREADS rows for each batch (and then
     start again to reuse the scratch buffers). */

  /* Buffer for temporarily storing Legendre function values */
  local double work_Lambda_a[MAX_NI], work_Lambda_b[MAX_NI];

  /* Simple scratch for auxiliary values */
  local double alpha[NTHREADS], beta[NTHREADS], gamma[NTHREADS];

  /* Buffer for sum-reduction. We use the one column padding trick to avoid
     memory bank conflicts. See work_sum_idx for indexing.
  */

  local double work_local_sum[NWARPS * NTHREADS * NVECS];
  local double work_warp_sum[NTHREADS * NVECS];

  /*  if (get_local_size(0) != NTHREADS) {
    printf("Assertion get_local_size(0) == NTHREADS failed\n");
    printf("FAIL %d", *(int*)0);
    }*/

  int iblock = get_global_id(0) / NTHREADS;

  local double *Lambda_prev, *Lambda_prevprev;

  /* Shift to our block */
  x_squared += iblock * ni;
  Lambda_0 += iblock * ni;
  Lambda_1 += iblock * ni;
  q += iblock * ni * NVECS;
  out += iblock * nk * NVECS;

  /* First two rows are special as they are dot products. Each thread
     only stores values for its own local consumption, so no need for
     barrier. */
  Lambda_prevprev = work_Lambda_a;
  Lambda_prev = work_Lambda_b;
  dot_and_copy(Lambda_0, q, Lambda_prevprev, work_warp_sum, ni);
  warp_sum_reduce(0, work_warp_sum, work_local_sum);
  dot_and_copy(Lambda_1, q, Lambda_prev, work_warp_sum, ni);
  warp_sum_reduce(1, work_warp_sum, work_local_sum);
  barrier(CLK_LOCAL_MEM_FENCE);
  inter_warp_sum(0, 2, work_local_sum, out, nk);
  /* Will hit barrier below before work_local_sum gets overwritten */

  for (int k_block_start = 2; k_block_start < nk; k_block_start += NTHREADS) {
    int k_block_len = min(NTHREADS, nk - k_block_start);
    /* Compute auxiliaries for this row-block */
    compute_recurrence_auxiliaries(m, lmin + 2 * k_block_start, NTHREADS,
                                   alpha, beta, gamma);
    barrier(CLK_LOCAL_MEM_FENCE);

    for (int k_offset = 0; k_offset != k_block_len; ++k_offset) {
      /* Row initialization*/
      double acc[NVECS];
      for (int j = 0; j != NVECS; ++j) {
        acc[j] = 0;
      }

      /* Process row. There may be less threads than columns, so for
         each row, let the threads stride across so that all columns are
         considered. */
      for (int i = ithread; i < ni; i += NTHREADS) {
        /* Recurrence */
        double Lambda_val =
          ((x_squared[i] + alpha[k_offset]) * beta[k_offset] * Lambda_prev[i] +
           gamma[k_offset] * Lambda_prevprev[i]);
        /* Save Lambda_cur in Lambda_prevprev; then we shuffle the
           buffer pointers after processing the entire row. */
        Lambda_prevprev[i] = Lambda_val;

        /* matvec */
        for (int j = 0; j != NVECS; ++j) {
          acc[j] += Lambda_val * q[j * ni + i];
        }
      }

      /* Done with row -- shuffle buffers */
      local double *tmp = Lambda_prevprev;
      Lambda_prevprev = Lambda_prev;
      Lambda_prev = tmp;

      /* Before heading to the next row, we do a within-warp reduction
         of the accumulators. This is necesarry lest work_local_sum
         becomes too big, and cheap enough as we do not need any
         thread synchronization. */
      for (int j = 0; j != NVECS; ++j) {
        work_warp_sum[j * NTHREADS + ithread] = acc[j];
      }
      warp_sum_reduce(k_offset, work_warp_sum, work_local_sum);
    }

    /* Done with row block. Do inter-warp accumulation and write to output
       before heading on to the next block. We let k=ithread, and have
       each thread sum over iwarp. */
    barrier(CLK_LOCAL_MEM_FENCE);
    inter_warp_sum(k_block_start, k_block_len,
                   work_local_sum, out, nk);
    /* Do not need another barrier here because the next iteration will
       hit the barrier after computing auxiliary values before it
       gets a chance to overwrite work_local_sum. */
  }

}
