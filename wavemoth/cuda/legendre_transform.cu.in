#include <stdio.h>

{{py:
def log2(x):
    r = -1
    while x != 0:
        x //= 2
        r += 1
    return r

def idiv(x, y): # Because / followed by / destroys emacs formatting
    return x.__floordiv__(y)
}}

#define NVECS {{nvecs}}
#define NTHREADS {{nthreads}}
#define APPROXIMATE_AUX 0
#define MAX_NI {{max_ni}}
#define WS {{warp_size}}
{{py: nwarps = nthreads // warp_size}}
#define NWARPS {{nwarps}}
{{default memory_bank_count = warp_size}}
#define K_CHUNK {{k_chunk}}
#define I_CHUNK {{i_chunk}}
{{py: ws = warp_size}}

/* Cast to int to make it easier to printf, as all our integers are
   then 32-bit. */
#define ithread ((int)threadIdx.x)
#define iblock ((int)blockIdx.x)

#define iwarp (ithread / WS)


/*****
 * Utilities
 *****/
/*
void print_array(constant const char *msg, local double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}

void print_array_glob(constant const char *msg, global double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}
*/

/* Multiply four 16-bit ints and produce 64-bit result safely. */
inline __device__ unsigned long prod4_16(unsigned short a, unsigned short b,
                              unsigned short c, unsigned short d) {
  return (unsigned long)((unsigned int)a * b) * ((unsigned int)c * d);
}


/*****
 * Auxiliary value computation
 *****/

__device__ double get_c_squared_num(unsigned short m, unsigned short l) {
  return prod4_16(l - m + 1, l - m + 2,
                  l + m + 1, l + m + 2);
}

__device__ double get_c_squared_den(unsigned short m, unsigned short l) {
  return prod4_16(2 * l + 1, 2 * l + 3,
                  2 * l + 3, 2 * l + 5);
}

__device__ double get_c(unsigned short m, unsigned short l) {
  /* A reciprocial square-root of the inverse whould be faster,
     but cost 1 ulp precision, and this step is very amortized. */
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#else
  return sqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#endif
}

__device__ double get_c_inv(unsigned short m, unsigned short l) {
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#else
  return sqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#endif
}

__device__ double get_d(unsigned short m, unsigned short l) {
  double num = 2 * l * (l + 1) - 2 * m * m - 1;
  double den = (2 * l - 1) * (2 * l + 3);
  return num / den;
}

__device__ void compute_recurrence_auxiliaries(int m, int lmin,
                                               double *alpha,
                                               double *beta,
                                               double *gamma) {
  /* Let half of the warps handle alpha, the rest beta/gamma; taking
     care to take both branches when NWARPS==1. */
  {{def alphapart()}}
      alpha[k] = -get_d(m, l - 2);
  {{enddef}}

  {{def betagammapart()}}
      double cpp = get_c(m, l - 4);
      double cp_inv = get_c_inv(m, l - 2);
      beta[k] = cp_inv;
      gamma[k] = -cpp * cp_inv;
  {{enddef}}

  {{if nthreads == warp_size}}
    for (int k = ithread; k < K_CHUNK; k += NTHREADS) {
      int l = lmin + 2 * k;
      {{alphapart()}}
      {{betagammapart()}}
    }
  {{else}}
    {{py: nthreads_mid = (idiv(nwarps, 2) * warp_size)}}
    if (ithread < {{nthreads_mid}}) {
      for (int k = ithread; k < K_CHUNK; k += {{nthreads_mid}}) {
        int l = lmin + 2 * k;
        {{alphapart()}}
      }
    } else {
      for (int k = ithread - {{nthreads_mid}}; k < K_CHUNK; k += {{nthreads - nthreads_mid}}) {
        int l = lmin + 2 * k;
        {{betagammapart()}}
      }
    }
  {{endif}}
}

/*****
 * Parallel tree reduction w/ test case
 *****/

{{py:
class ReductionContext(object):
    def __init__(self, nextrow_func, store_func):
        self.all_buffers = set()
        self.free_buffers = set()
        self.nextrow_func = nextrow_func
        self.store_func = store_func

    def allocate_buf(self):
        if len(self.free_buffers) == 0:
            buffer = 'reduce_buf_%d' % len(self.all_buffers)
            self.all_buffers.add(buffer)
            self.free_buffers.add(buffer)
        return self.free_buffers.pop()

    def deallocate_buf(self, buffer):
        self.free_buffers.add(buffer)

}}

{{def reduce_worker(ctx, row_start, row_stop, out_vname)}}
  {{if row_stop - row_start == 1}}
    {{ctx.nextrow_func(row_start, out_vname)}}
  {{else}}
    {{py: buf_vname = ctx.allocate_buf()}}
    {{py: nrows = row_stop - row_start}}
    {{reduce_worker(ctx, row_start, row_start + idiv(nrows, 2), buf_vname)}}
    {{reduce_worker(ctx, row_start + idiv(nrows, 2), row_stop, '(%s + WS)' % buf_vname)}}

    /* Do pairwise reduction of values in buf and store result in out.
       For the final output, we call user code to do the store. */
    {
      double _reduced_val = {{buf_vname}}[reduce_idx] + {{buf_vname}}[reduce_idx + 2];
      {{if out_vname is None}} /* first level */
        int _reduced_j = ithread % 2;
        int _reduced_k = ((ithread % WS) / 2);
        {{ctx.store_func('_reduced_j', '_reduced_k', '_reduced_val')}}
      {{else}} /* inner level, store results for parent level */
        {{out_vname}}[ithread % WS] = _reduced_val;
      {{endif}}
    }
    {{py: ctx.deallocate_buf(buf_vname)}}
  {{endif}}
{{enddef}}

{{def parallel_tree_reduction(*args)}}
  {{py: ctx = ReductionContext(*args)}}

  {{py: body = reduce_worker(ctx, 0, 16, None)}}

  /* Buffers for intra-warp communication. They are segmented into one
     independent area for each warp of size 2 * WS; no cross-warp talk. */
  {{for buf in ctx.all_buffers}}
    __shared__ volatile double _{{buf}}[2 * NTHREADS];
    volatile double *{{buf}} = _{{buf}} + 2 * iwarp * WS;
  {{endfor}}
  int pair_idx = (ithread % WS) / 2;
  int reduce_idx = 4 * pair_idx + ithread % 2;
  {{body}}
{{enddef}}

{{py: assert nvecs % 2 == 0}}
__global__ void test_reduce_kernel(double *output, int repeat) {
  /* Generates array (nvecs, 16, nwarps) in output, consisting
     of sum_{over warp} ( ithread * irow * (ivec + 1)) ),
     using the same parallel reduction algorithm as we use in
     transpose_legendre_transform. */

  /* Move to block */
  output += NTHREADS * iblock;

  __shared__ double buffer[NTHREADS];

  /* Zero output -- note that 2*16*nwarps == nthreads. */
  buffer[ithread] = 0;
  __syncthreads();

  /* Registers for loading row of input*/
  {{def nextrow(rowidx, outbuf)}}
    /* Load next row into registers -- in transpose_legendre_transform,
       this step is replaced by computation. The "computation order" must
       be arranged so that vec-number is reversed for every other thread... */
  {
    int odd = ithread % 2;
    int j_mine = odd;
    int j_other = 1 - odd;
    double mine = ithread * {{rowidx}} * (j_mine + 1);
    double to_other = ithread * {{rowidx}} * (j_other + 1);
    /* Pair-of-two sum-reduction. Result is stored
       interleaved: (j=0, j=1, j=0, j=1, ...) */
    {{outbuf}}[ithread % 32 + 1 - 2 * odd] = to_other; /* Send to neighbour thread */
    {{outbuf}}[ithread % 32] += mine; /* Accumulate with what neighbour stored */
  }
  {{enddef}}
  {{def store(vec_code, row_code, value_code)}}
  buffer[{{vec_code}} * 16 * NWARPS + {{row_code}} * NWARPS + iwarp] += {{value_code}};
  {{enddef}}

  for (int rep = 0; rep != repeat; ++rep) {
    {{parallel_tree_reduction(nextrow, store)}}
  }

  __syncthreads();
  output[ithread] = buffer[ithread];

}




/*****
 * Main code
 *****/

{{py:
kernel_or_not = [('__global__', '_kernel'),
                 ('__device__', '')]
}}
     
/* Do a dot product of the 1D vector P with NVECS vectors
   stored in q, and simultaneously copy P into local memory
   P_local. */
{{for spec, suffix in kernel_or_not}}
{{spec}} void dot_and_copy{{suffix}}(const double *P,
                                     const double *q,
                                     double *P_copy,
                                     double *work_sum,
                                     int ni) {
  double s[NVECS];
  for (int j = 0; j != NVECS; ++j) {
    s[j] = 0;
  }
  for (int i = ithread; i < ni; i += NTHREADS) {
    double Pval = P[i];
    P_copy[i] = Pval;
    for (int j = 0; j != NVECS; ++j) {
      s[j] += Pval * q[j * ni + i];
    }
  }
  for (int j = 0; j != NVECS; ++j) {
    work_sum[j * NTHREADS + ithread] = s[j];
  }
}
{{endfor}}

/* thread_sums[j, ithread] is summed over the threads in each warp and
   result placed in warp_sums[iwarp, k_offset, j].
   Contents of thread_sums is destructed in the process. */
{{for spec, suffix in kernel_or_not}}
{{spec}} void warp_sum_reduce{{suffix}}(int k_offset,
                                        double *thread_sums,
                                        double *warp_sums) {
    /* TODO: Use parallelism to work on multiple vectors in parallel... */
  {{for p in range(log2(warp_size))}}
  {{py:stride = 2**p}}
  if (ithread % {{2 * stride}} == 0) {
    for (int j = 0; j != NVECS; ++j) {
      {{if 2 * stride == warp_size}}
      {{# Write to warp_sums }}
      warp_sums[iwarp * K_CHUNK * NVECS + k_offset * NVECS + j] += 
        (thread_sums[j * NTHREADS + ithread] +
        thread_sums[j * NTHREADS + ithread + {{stride}}]);
      {{else}}
      {{# Write temporary result to thread_sums}}
      thread_sums[j * NTHREADS + ithread] += thread_sums[j * NTHREADS + ithread + {{stride}}];
    {{endif}}
    }
  }
  {{endfor}}
}
{{endfor}}


/* Sum up a number of output buffers (one per warp) in
   `work_local_sum`, and put the result in `out`.  `work_local_sum`
   is a 3D array indexed by [iwarp, k_offset, j] and dimensions
   (NWARPS, K_CHUNK, NVECS).
*/

{{for spec, suffix in kernel_or_not}}
{{spec}} void inter_warp_sum{{suffix}}(int k_start, int nk,
                                       double *work_local_sum,
                                       double *out,
                                       int out_stride) {
  /* Have threads divide evenly over the different vectors, gracefully
     handling different nk. */
  for (int idx = ithread; idx < NVECS * nk; idx += NTHREADS) {
    double s = 0;
    for (int t = 0; t != NWARPS; ++t) {
      s += work_local_sum[t * K_CHUNK * NVECS + idx]; /*idx == k * NVECS + j*/
    }
    int j = idx % NVECS;
    int k = idx / NVECS;
    out[j * out_stride + k_start + k] = s;
  }
}
{{endfor}}

/* Zero the accumulation buffer on a per-warp basis, so that no
   barrier is needed. work_local_sum is a 3D array indexed by [iwarp,
   k_offset, j] */
__device__ void zero_accumulation_buffer(double *work_local_sum) {
  for (int idx = ithread % WS; idx < NVECS * K_CHUNK; idx += WS) {
    work_local_sum[iwarp * K_CHUNK * NVECS + idx] = 0;
  }
}

/*****
 * Kernel entry points
 *****/

__global__ void transpose_legendre_transform(int m, int lmin, int nk, int ni,
                                             const double *x_squared,
                                             const double *Lambda_0,
                                             const double *Lambda_1,
                                             const double *q,
                                             double *work,
                                             double *out,
                                             int zero) {
  /* Scratch for auxiliary values */
  __shared__ double alpha[K_CHUNK], beta[K_CHUNK], gamma[K_CHUNK];

  /* Scratch for sum-reduction. We use the one column padding trick to
     avoid memory bank conflicts. See work_sum_idx for indexing.  */

  __shared__ double work_local_sum[NWARPS * K_CHUNK * NVECS];
//  __shared__ double work_warp_sum[NTHREADS * NVECS];

  if ((nk - 2) % 16 != 0) {
    printf("Assertion (nk - 2) % 16 == 0 failed\n");
    printf("FAIL %d", *(int*)0);
  }

  /* Buffer for temporarily storing Legendre function values */
  double *Lambda_pp = work + (2 * iblock) * MAX_NI;
  double *Lambda_p = work + (2 * iblock + 1) * MAX_NI;

  /* Shift to our block */
  x_squared += iblock * ni;
  Lambda_0 += iblock * ni;
  Lambda_1 += iblock * ni;
  q += iblock * ni * NVECS;
  out += iblock * nk * NVECS;

  /* First two rows are special as they are dot products. Each thread
     only stores values for its own local consumption, so no need for
     barrier. */
  /*zero_accumulation_buffer(work_local_sum);
  dot_and_copy(Lambda_0, q, Lambda_pp, work_warp_sum, ni);
  warp_sum_reduce(0, work_warp_sum, work_local_sum);
  dot_and_copy(Lambda_1, q, Lambda_p, work_warp_sum, ni);
  warp_sum_reduce(1, work_warp_sum, work_local_sum);
  __syncthreads();
  inter_warp_sum(0, 2, work_local_sum, out, nk);
*/
  for (int j = 0; j != NVECS; ++j) {
    out[j * nk + 0] = 0;
    out[j * nk + 1] = 0;
  }
  for (int i = 0; i != ni / I_CHUNK; ++i) {
    Lambda_pp[i * NTHREADS] = Lambda_0[i * NTHREADS];
    Lambda_p[i * NTHREADS] = Lambda_1[i * NTHREADS];
  }

  /* No barrier: Will hit barrier below before work_local_sum gets
     overwritten again, and we only write once to each element in
     out. */


  /* Blocking approach: Process blocks of size (K_CHUNK, I_CHUNK * NTHREADS).
     Within each block, local memory contains data needed for rows (k-axis),
     and registers contains data needed for columns (i-axis).
     The blocks are processed in row-wise order.
  */
  for (int k_block_start = 2; k_block_start < nk; k_block_start += K_CHUNK) {
    int k_block_len = min(K_CHUNK, nk - k_block_start);
    /* Compute auxiliaries for this row-block */
    compute_recurrence_auxiliaries(m, lmin + 2 * k_block_start,
                                   alpha, beta, gamma);

    __syncthreads();
    /* Zero accumulation scratch buffer */
    zero_accumulation_buffer(work_local_sum);

    /* Process all blocks on this block-row */
    for (int i_block_start = 0; i_block_start < ni; i_block_start += I_CHUNK * NTHREADS) {
      int i_block_len = min(NTHREADS * I_CHUNK, ni - i_block_start);

      /* Read Lambdas, x_squared, and q from global memory into
         registers for this block.  Shuffle q's so that for odd
         ithread, j=1 corresponds to rj=0; i.e., the odd threads
         compute j=1 before j=0. This is convenient when doing the
         initial 32->16 column reduction below.
       */
      int i_offset = i_block_start + ithread;
      int odd = ithread % 2;
      {{for ri in range(i_chunk)}}
        double Lambda_pp_{{ri}} = Lambda_pp[i_offset + {{nthreads * ri}}];
        double Lambda_p_{{ri}} = Lambda_p[i_offset + {{nthreads * ri}}];
        double x_squared_{{ri}} = x_squared[i_offset + {{nthreads * ri}}];
        {{for rj in range(nvecs)}}
          double q_{{ri}}_{{rj}} = q[(odd - {{rj}}) * ni + i_offset + {{nthreads * ri}}];
        {{endfor}}
      {{endfor}}

      /* Code to be used to compute each row, including 32->16 reduction. */
      {{def nextrow(kt, outbuf)}}
      {
        double acc_0 = 0, acc_1 = 0;
        {{for ri in range(i_chunk)}}
        {
          double Lambda_val =
            ((x_squared_{{ri}} + alpha[k_offset + {{kt}}]) * beta[k_offset + {{kt}}] * Lambda_p_{{ri}} +
             gamma[k_offset + {{kt}}] * Lambda_pp_{{ri}});
          {{for rj in range(nvecs)}}
            acc_{{rj}} += Lambda_val * q_{{ri}}_{{rj}};
          {{endfor}}

          Lambda_pp_{{ri}} = Lambda_p_{{ri}};
          Lambda_p_{{ri}} = Lambda_val;
        }
        {{endfor}}
        
        /* Do 32->16 reduction and store result in outbuf. Note that
           acc_t corresponds to j=|t-odd|. */
        {
          {{outbuf}}[ithread % 32] = acc_1;
          acc_0 += {{outbuf}}[ithread % 32 + 1 - 2 * odd];
          {{outbuf}}[ithread % 32] = acc_0;
        }
      }
      {{enddef}}

      /* Code to be used to store results of intra-warp reduction. Note that
         we accumulate results of this block. */
      {{def store(vec_code, row_code, value_code)}}
      work_local_sum[iwarp * K_CHUNK * NVECS + (k_offset + {{row_code}}) * NVECS + {{vec_code}}] +=
        {{value_code}};
      {{enddef}}

      /* Generate generation-and-reduction code */
      for (int k_offset = 0; k_offset != K_CHUNK; k_offset += 16) {
        /* Computation and intra-warp reduction */
        {{parallel_tree_reduction(nextrow, store)}}
        /* Cross-warp reduction */
        {{#py: assert nthreads == 64}}
        __syncthreads();
      }

    }

    /* Done with row block. Do inter-warp accumulation and write to output
       before heading on to the next block. We let k=ithread, and have
       each thread sum over iwarp. */
    __syncthreads();
    inter_warp_sum(k_block_start, k_block_len,
                   work_local_sum, out, nk);
    /* Do not need another barrier here because the next iteration will
       hit the barrier after computing auxiliary values before it
       gets a chance to overwrite work_local_sum. */

  }

}
