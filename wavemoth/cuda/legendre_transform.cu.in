#include <stdio.h>

{{py:
def log2(x):
    r = -1
    while x != 0:
        x //= 2
        r += 1
    return r

def idiv(x, y): # Because / followed by / destroys emacs formatting
    return x.__floordiv__(y)
}}

#define NVECS {{nvecs}}
#define NTHREADS {{nthreads}}
#define APPROXIMATE_AUX 0
#define MAX_NI {{max_ni}}
#define WS {{warp_size}}
{{py: nwarps = nthreads // warp_size}}
#define NWARPS {{nwarps}}
{{default memory_bank_count = warp_size}}
#define K_CHUNK {{k_chunk}}
#define I_CHUNK {{i_chunk}}
{{py: ws = warp_size}}

/* Cast to int to make it easier to printf, as all our integers are
   then 32-bit. */
#define ithread ((int)threadIdx.x)
#define iblock ((int)blockIdx.x)

#define iwarp (ithread / WS)

#define _UNASSIGNED 0xffffffff01234567UL
#define UNASSIGNED __longlong_as_double(_UNASSIGNED)
#define IS_UNASSIGNED(x) (__double_as_longlong(x) == _UNASSIGNED)

{{def check(cond, msg='assertion failed')}}
 if (!{{int(cond)}}) {
   printf("{{msg}}\n");
   return;
 }
{{enddef}}


/*****
 * Utilities
 *****/

__device__ void print_array(const char *msg, double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}
/*
void print_array_glob(constant const char *msg, global double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}
*/

/* Multiply four 16-bit ints and produce 64-bit result safely. */
inline __device__ unsigned long prod4_16(unsigned short a, unsigned short b,
                              unsigned short c, unsigned short d) {
  return (unsigned long)((unsigned int)a * b) * ((unsigned int)c * d);
}


/*****
 * Auxiliary value computation
 *****/

__device__ double get_c_squared_num(unsigned short m, unsigned short l) {
  return prod4_16(l - m + 1, l - m + 2,
                  l + m + 1, l + m + 2);
}

__device__ double get_c_squared_den(unsigned short m, unsigned short l) {
  return prod4_16(2 * l + 1, 2 * l + 3,
                  2 * l + 3, 2 * l + 5);
}

__device__ double get_c(unsigned short m, unsigned short l) {
  /* A reciprocial square-root of the inverse whould be faster,
     but cost 1 ulp precision, and this step is very amortized. */
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#else
  return sqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#endif
}

__device__ double get_c_inv(unsigned short m, unsigned short l) {
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#else
  return sqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#endif
}

__device__ double get_d(unsigned short m, unsigned short l) {
  double num = 2 * l * (l + 1) - 2 * m * m - 1;
  double den = (2 * l - 1) * (2 * l + 3);
  return num / den;
}

__device__ void compute_recurrence_auxiliaries(int m, int lmin,
                                               double *alpha,
                                               double *beta,
                                               double *gamma) {
  /* Let half of the warps handle alpha, the rest beta/gamma; taking
     care to take both branches when NWARPS==1. */
  {{def alphapart()}}
      alpha[k] = -get_d(m, l - 2);
  {{enddef}}

  {{def betagammapart()}}
      double cpp = get_c(m, l - 4);
      double cp_inv = get_c_inv(m, l - 2);
      beta[k] = cp_inv;
      gamma[k] = -cpp * cp_inv;
  {{enddef}}

  {{if nwarps == 1 or k_chunk % nthreads == 1}}
    for (int k = ithread; k < K_CHUNK; k += NTHREADS) {
      int l = lmin + 2 * k;
      {{alphapart()}}
      {{betagammapart()}}
    }
  {{else}}
    {{py: nthreads_mid = (idiv(nwarps, 2) * warp_size)}}
    if (ithread < {{nthreads_mid}}) {
      for (int k = ithread; k < K_CHUNK; k += {{nthreads_mid}}) {
        int l = lmin + 2 * k;
        {{alphapart()}}
      }
    } else {
      for (int k = ithread - {{nthreads_mid}}; k < K_CHUNK; k += {{nthreads - nthreads_mid}}) {
        int l = lmin + 2 * k;
        {{betagammapart()}}
      }
    }
  {{endif}}
}

/*****
 * Parallel tree reduction w/ test case
 *****/

{{py:
class ReductionContext(object):
    def __init__(self, nextrow_func, store_func):
        self.all_buffers = set()
        self.free_buffers = set()
        self.nextrow_func = nextrow_func
        self.store_func = store_func

    def allocate_buf(self):
        #self.all_buffers.add('reduce_buf_1')
        #return 'reduce_buf_1'
        if len(self.free_buffers) == 0:
            buffer = 'reduce_buf_%d' % len(self.all_buffers)
            self.all_buffers.add(buffer)
            self.free_buffers.add(buffer)
        return self.free_buffers.pop()

    def deallocate_buf(self, buffer):
        self.free_buffers.add(buffer)

}}

{{def reduce_worker(ctx, row_start, row_stop, out_vname, is_top, store_func)}}
  {{py: out_slice_vname = out_vname if is_top or out_vname is None else '(%s + WS)' % out_vname}}
  {{if row_stop - row_start == 1}}
    {{ctx.nextrow_func(row_start, out_slice_vname)}}
  {{else}}
    /* Pass on output buffer to children if is_top, else allocate a new
       one. */
    {{py: buf_vname = out_vname if is_top else ctx.allocate_buf()}}
    {{py: nrows = row_stop - row_start}}
    {{reduce_worker(ctx, row_start, row_start + idiv(nrows, 2), buf_vname, True, None)}}
    {{reduce_worker(ctx, row_start + idiv(nrows, 2), row_stop, buf_vname, False, None)}}

    /* Do pairwise reduction of values in buf and store result in out.
       For the final output, we call user code to do the store. */
    {
      double _reduced_val = {{buf_vname}}[reduce_idx] + {{buf_vname}}[reduce_idx + 2];
      {{if store_func is not None}}
        /* we are asked to store reduced value directly to target */
        int _reduced_j = ithread % 2;
        int _reduced_k = ((ithread % WS) / 2);
        {{store_func('_reduced_j', '_reduced_k', '_reduced_val')}}
      {{else}}
        /* inner level, store results for parent level */
        {{out_slice_vname}}[ithread % WS] = _reduced_val;
      {{endif}}
    }
    {{py: if not is_top: ctx.deallocate_buf(buf_vname)}}
  {{endif}}
{{enddef}}

{{def parallel_tree_reduction(*args)}}
/*
  If nwarps == 1, we pass the store_func to reduce_worker so that the final
  intra-warp reduction stores values. Otherwise, we proceed to inter-warp
  reduction and call store_func at the end of that.
*/
  {{py: ctx = ReductionContext(*args)}}
  {{py: root_buf = ctx.allocate_buf()}}
  {{py: body = reduce_worker(ctx, 0, 16, root_buf, True, ctx.store_func if nwarps == 1 else None)}}

  /* Buffers for intra-warp communication. They are segmented into one
     independent area for each warp of size 2 * WS; no cross-warp talk. */
  {{for buf in ctx.all_buffers}}
    __shared__ volatile double _{{buf}}[2 * NTHREADS];
    volatile double *{{buf}} = _{{buf}} + 2 * iwarp * WS;
  {{endfor}}
  int pair_idx = (ithread % WS) / 2;
  int reduce_idx = 4 * pair_idx + ithread % 2;
  /* Intra-warp reduction */
  {{body}}
  {{if nwarps > 1}}
    /* Inter-warp reduction within root_buf. The following is not an
       optimal work-load distribution scheme, but we seem to be targeting
       nwarps==2 at the time of writing... */
    __syncthreads();
    if (iwarp == 0) {
      double _reduced_val = 0;
      for (int t = 0; t != NWARPS; ++t) {
        _reduced_val += (_{{root_buf}} + t * 2 * WS)[ithread];
      }
      int _reduced_j = ithread % 2;
      int _reduced_k = ithread / 2;
      {{ctx.store_func('_reduced_j', '_reduced_k', '_reduced_val')}}
    }
    __syncthreads();
  {{endif}}
{{enddef}}

{{py: assert nvecs % 2 == 0}}
__global__ void test_reduce_kernel(double *output, int repeat) {
  /* Generates array (nvecs, 16, nwarps) in output, consisting
     of sum_{over warp} ( ithread * irow * (ivec + 1)) ),
     using the same parallel reduction algorithm as we use in
     transpose_legendre_transform. */

  /* Move to block */
  output += NTHREADS * iblock;

  __shared__ double buffer[NTHREADS];

  /* Zero output -- note that 2*16*nwarps == nthreads. */
  buffer[ithread] = 0;
  __syncthreads();

  /* Registers for loading row of input*/
  {{def nextrow(rowidx, outbuf)}}
    /* Load next row into registers -- in transpose_legendre_transform,
       this step is replaced by computation. The "computation order" must
       be arranged so that vec-number is reversed for every other thread... */
  {
    int odd = ithread % 2;
    int j_mine = odd;
    int j_other = 1 - odd;
    double mine = ithread * {{rowidx}} * (j_mine + 1);
    double to_other = ithread * {{rowidx}} * (j_other + 1);
    /* Pair-of-two sum-reduction. Result is stored
       interleaved: (j=0, j=1, j=0, j=1, ...) */
    {{outbuf}}[ithread % 32 + 1 - 2 * odd] = to_other; /* Send to neighbour thread */
    {{outbuf}}[ithread % 32] += mine; /* Accumulate with what neighbour stored */
  }
  {{enddef}}
  {{def store(vec_code, row_code, value_code)}}
  buffer[{{vec_code}} * 16 * NWARPS + {{row_code}} * NWARPS + iwarp] += {{value_code}};
  {{enddef}}

  for (int rep = 0; rep != repeat; ++rep) {
    {{parallel_tree_reduction(nextrow, store)}}
  }

  __syncthreads();
  output[ithread] = buffer[ithread];

}


/*****
 * Transposed Legendre transform
 *****/

__global__ void transpose_legendre_transform(int m, int lmin, int nk, int ni,
                                             const double *x_squared,
                                             const double *Lambda_0,
                                             const double *Lambda_1,
                                             const unsigned short *i_stops,
                                             const double *q,
                                             double *work,
                                             double *out,
                                             int zero) {
  /* Scratch for auxiliary values */
  __shared__ double alpha[K_CHUNK], beta[K_CHUNK], gamma[K_CHUNK];
  __shared__ unsigned short i_stops_chunk[K_CHUNK];
  __shared__ double out_chunk[NVECS * K_CHUNK];

  /* Code to be used to compute each row, including 32->16 reduction. */
  {{def nextrow(kt, outbuf)}}
  {
    double acc_0 = 0, acc_1 = 0;
    double Lambda_val = 0;
    {{for ri in range(i_chunk)}}
    {
      int i = i_offset + {{nthreads * ri}};
      int kc = k_offset + {{kt}};
//      if (iblock == 0 && ithread == 0) {printf("%d %d\n", i, kc);}
      if (i < i_stops_chunk[kc]) {
        if (IS_UNASSIGNED(Lambda_p_{{ri}})) {
          /* Have not started this column at all yet */
          Lambda_val = Lambda_0[i];
          //if (iblock == 0 && ithread == 0) printf("LOAD 0 %e\n", Lambda_val);
        } else if (IS_UNASSIGNED(Lambda_pp_{{ri}})) {
          /* Lambda_p != 0, Lambda_pp == 0 => Started in the previous iteration */
          Lambda_val = Lambda_1[i];
          //if (iblock == 0 && ithread == 0) printf("LOAD 1 %e\n", Lambda_val);
        } else {
          Lambda_val =
            ((x_squared_{{ri}} + alpha[kc]) * beta[kc] * Lambda_p_{{ri}} +
             gamma[kc] * Lambda_pp_{{ri}});
          //if (iblock == 0 && ithread == 0) printf("COMPUTE 1 %e\n", Lambda_val);
        }
      }
      {{for rj in range(nvecs)}}
        acc_{{rj}} += Lambda_val * q_{{ri}}_{{rj}};
      {{endfor}}

      Lambda_pp_{{ri}} = Lambda_p_{{ri}};
      Lambda_p_{{ri}} = Lambda_val;
    }
    {{endfor}}
        
    /* Do 32->16 reduction and store result in outbuf. Note that
       acc_t corresponds to j=|t-odd|. */
    {
      {{outbuf}}[ithread % 32 + 1 - 2 * odd] = acc_1;
      {{outbuf}}[ithread % 32] += acc_0;
    }
  }
  {{enddef}}

  /* Code to be used to store final reduction results. Only executed for iwarp==0. */
  {{def store(vec_code, row_code, value_code)}}
  out_chunk[k_offset * NVECS + ithread] += {{value_code}};
  {{enddef}}

  for (int idx = ithread; idx < NVECS * K_CHUNK; idx += NTHREADS) {
    out_chunk[idx] = 0;
  }

  /* Buffer for temporarily storing Legendre function values */
  volatile double *Lambda_pp = work + (2 * iblock) * MAX_NI;
  volatile double *Lambda_p = work + (2 * iblock + 1) * MAX_NI;

  /* Shift to our block */
  x_squared += iblock * ni;
  Lambda_0 += iblock * ni;
  Lambda_1 += iblock * ni;
  i_stops += iblock * nk;
  q += iblock * ni * NVECS;
  out += iblock * nk * NVECS;

  /* First two rows are special as they are dot products. Each thread
     only stores values for its own local consumption, so no need for
     barrier. */
  /*zero_accumulation_buffer(work_local_sum);
  dot_and_copy(Lambda_0, q, Lambda_pp, work_warp_sum, ni);
  warp_sum_reduce(0, work_warp_sum, work_local_sum);
  dot_and_copy(Lambda_1, q, Lambda_p, work_warp_sum, ni);
  warp_sum_reduce(1, work_warp_sum, work_local_sum);
  __syncthreads();
  inter_warp_sum(0, 2, work_local_sum, out, nk);
*/
  for (int j = 0; j != NVECS; ++j) {
    out[j * nk + 0] = 0;
    out[j * nk + 1] = 0;
  }

  for (int idx = ithread; idx < ni; idx += NTHREADS) {
    Lambda_pp[idx] = UNASSIGNED;//Lambda_0[idx];
    Lambda_p[idx] = UNASSIGNED;//Lambda_1[idx];
  }


  /* No barrier: Will hit barrier below before work_local_sum gets
     overwritten again, and we only write once to each element in
     out. */


  /* Blocking approach: Process blocks of size (K_CHUNK, I_CHUNK * NTHREADS).
     Within each block, local memory contains data needed for rows (k-axis),
     and registers contains data needed for columns (i-axis).
     The blocks are processed in row-wise order.
  */
  for (int k_block_start = 0; k_block_start < nk; k_block_start += K_CHUNK) {
    /* Put values invariant over row-block in shared memory:
         - Compute auxiliaries
         - Copy i_stops 
    */
    compute_recurrence_auxiliaries(m, lmin + 2 * k_block_start,
                                   alpha, beta, gamma);
    for (int idx = ithread; idx < K_CHUNK; idx += NTHREADS) {
      i_stops_chunk[idx] = i_stops[k_block_start + idx];
    }
    __syncthreads();
    
    /* Process all blocks on this block-row */
    for (int i_block_start = 0; i_block_start < ni; i_block_start += I_CHUNK * NTHREADS) {
      int i_block_len = min(NTHREADS * I_CHUNK, ni - i_block_start);

      /* Read Lambdas, x_squared, and q from global memory into
         registers for this block.  Shuffle q's so that for odd
         ithread, j=1 corresponds to rj=0; i.e., the odd threads
         compute j=1 before j=0. This is convenient when doing the
         initial 32->16 column reduction below.
       */
      int i_offset = i_block_start + ithread;
      int odd = ithread % 2;
      {{for ri in range(i_chunk)}}
        double Lambda_pp_{{ri}}, Lambda_p_{{ri}};
        double x_squared_{{ri}};
        double {{', '.join(['q_%d_%d' % (ri, rj) for rj in range(nvecs)])}};
        {
          int i = i_offset + {{nthreads * ri}};
          Lambda_pp_{{ri}} = Lambda_pp[i];
          Lambda_p_{{ri}} = Lambda_p[i];
          x_squared_{{ri}} = x_squared[i];
          {{for rj in range(nvecs)}}
            q_{{ri}}_{{rj}} = q[({{'odd' if rj == 0 else '1 - odd'}}) * ni + i];
          {{endfor}}
        }
      {{endfor}}

      /* Process all rows, using generation-and-reduction code.
         See above for definition of nextrow and store; essentially,
         `nextrow` computes for next k, while `store` accumulates in
         the `out_chunk` buffer of shape (NVECS, K_CHUNK) .

         We always work in chunks of 16 rows (as that is what the
         reduction code support), but then avoid copying from buffer
         to out beyond nk at the end of the block row.
      */
      int k_block_len = min(K_CHUNK, nk - k_block_start);
      for (int k_offset = 0; k_offset < k_block_len; k_offset += 16) {
        {{parallel_tree_reduction(nextrow, store)}}
      }

      /* Persist Lambda's */
      {{for ri in range(i_chunk)}}
        Lambda_pp[i_offset + {{nthreads * ri}}] = Lambda_pp_{{ri}};
        Lambda_p[i_offset + {{nthreads * ri}}] = Lambda_p_{{ri}};
      {{endfor}}
    }

    /* Done with row block. Copy results from out_chunk to out, AND
       zero out_chunk. Note: Each iteration of parallel_tree_reduction
       ends with __syncthreads(), so threads are synced at this
       point. */
    __syncthreads();
    {{check(nvecs == nwarps == 2, 'not (nvecs == nwarps == 2)')}}
    {
      /* Values in out_chunk are stored in strided order [j=0, j=1, j=1, ...]. */
      int j = ithread % NVECS;
      for (int k_offset = 0; k_offset != K_CHUNK; k_offset += NTHREADS / NVECS) {
        int kt = ithread / 2;
        int k = k_block_start + k_offset + kt;
        int idx = (k_offset + kt) * NVECS + j;
        if (k < nk) {
          out[j * nk + k] = out_chunk[idx];
        }
        out_chunk[idx] = 0;
      }
    }
    /* Do not need another barrier here because the next iteration will
       hit the barrier after computing auxiliary values before it
       gets a chance to overwrite any accumulation buffers. */
  }

}
