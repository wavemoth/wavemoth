#include <stdio.h>

{{py:
def log2(x):
    r = -1
    while x != 0:
        x //= 2
        r += 1
    return r

def idiv(x, y): # Because / followed by / destroys emacs formatting
    return x.__floordiv__(y)

def round_up_to(x, mod):
    if x % mod > 0:
        x += mod - x % mod
    return x

}}

#define NVECS {{nvecs}}
#define NTHREADS {{nthreads}}
#define APPROXIMATE_AUX 0
#define MAX_NI {{max_ni}}
#define WS {{warp_size}}
{{py: nwarps = nthreads // warp_size}}
#define NWARPS {{nwarps}}
{{default skip_kernels = []}}
#define K_CHUNK {{k_chunk}}
#define I_CHUNK {{i_chunk}}
{{py:ws = warp_size}}

{{py:
reduction_buffers = ['reduction_buffer%d' % i for i in range(4)]
}}

#define MAX_NCHUNKS_I {{idiv(round_up_to(max_ni, i_chunk * nthreads), i_chunk * nthreads)}}

/* Cast to int to make it easier to printf, as all our integers are
   then 32-bit. */
#define ithread ((int)threadIdx.x)

#define iwarp (ithread / WS)

#define _UNASSIGNED 0xffffffff01234567UL
#define UNASSIGNED __longlong_as_double(_UNASSIGNED)
#define IS_UNASSIGNED(x) (__double_as_longlong(x) == _UNASSIGNED)

{{def check(cond, msg='assertion failed')}}
 if (!{{int(cond)}}) {
   printf("{{msg}}\n");
   return;
 }
{{enddef}}


/* Hack: Substitute function name with if-test + name. ugh;
   todo: look up vararg macros*/
#define printonce if (ithread == 0 && blockIdx.x == 0 && blockIdx.y == 0) printf

typedef unsigned short ushort;

/*****
 * Utilities
 *****/

__device__ void print_array(const char *msg, double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}
/*
void print_array_glob(constant const char *msg, global double* arr, int len) {
  int i;
  printf("%s ", msg);
  for (i = 0; i != len; ++i) {
    printf("%e ", arr[i]);
  }
  printf("\n");
}
*/

/* Multiply four 16-bit ints and produce 64-bit result safely. */
inline __device__ unsigned long prod4_16(ushort a, ushort b,
                              ushort c, ushort d) {
  return (unsigned long)((unsigned int)a * b) * ((unsigned int)c * d);
}

inline __device__ unsigned int round_up_to(unsigned int x, unsigned int mod) {
  if (x % mod != 0) {
    x += (mod - x % mod);
  }
  return x;
}



/*****
 * Auxiliary value computation
 *****/

__device__ double get_c_squared_num(ushort m, ushort l) {
  return prod4_16(l - m + 1, l - m + 2,
                  l + m + 1, l + m + 2);
}

__device__ double get_c_squared_den(ushort m, ushort l) {
  return prod4_16(2 * l + 1, 2 * l + 3,
                  2 * l + 3, 2 * l + 5);
}

__device__ double get_c(ushort m, ushort l) {
  /* A reciprocial square-root of the inverse whould be faster,
     but cost 1 ulp precision, and this step is very amortized. */
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#else
  return sqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#endif
}

__device__ double get_c_inv(ushort m, ushort l) {
#if APPROXIMATE_AUX
  return rsqrt(get_c_squared_num(m, l) / get_c_squared_den(m, l));
#else
  return sqrt(get_c_squared_den(m, l) / get_c_squared_num(m, l));
#endif
}

__device__ double get_d(ushort m, ushort l) {
  double num = 2 * l * (l + 1) - 2 * m * m - 1;
  double den = (2 * l - 1) * (2 * l + 3);
  return num / den;
}

__device__ void compute_recurrence_auxiliaries(int m, int lmin,
                                               double *alpha,
                                               double *beta,
                                               double *gamma) {
  /* Let half of the warps handle alpha, the rest beta/gamma; taking
     care to take both branches when NWARPS==1. */
  {{def alphapart()}}
      alpha[k] = -get_d(m, l - 2);
  {{enddef}}

  {{def betagammapart()}}
      double cpp = get_c(m, l - 4);
      double cp_inv = get_c_inv(m, l - 2);
      beta[k] = cp_inv;
      gamma[k] = -cpp * cp_inv;
  {{enddef}}

  {{if nwarps == 1 or k_chunk % nthreads == 1}}
    for (int k = ithread; k < K_CHUNK; k += NTHREADS) {
      int l = lmin + 2 * k;
      {{alphapart()}}
      {{betagammapart()}}
    }
  {{else}}
    {{py: nthreads_mid = (idiv(nwarps, 2) * warp_size)}}
    if (ithread < {{nthreads_mid}}) {
      for (int k = ithread; k < K_CHUNK; k += {{nthreads_mid}}) {
        int l = lmin + 2 * k;
        {{alphapart()}}
      }
    } else {
      for (int k = ithread - {{nthreads_mid}}; k < K_CHUNK; k += {{nthreads - nthreads_mid}}) {
        int l = lmin + 2 * k;
        {{betagammapart()}}
      }
    }
  {{endif}}
}

/*****
 * Parallel tree reduction w/ test case
 *****/

{{py:
class ReductionContext(object):
    def __init__(self, nextrow_func, store_func, buffers):
        self.free_buffers = set(buffers)
        self.nextrow_func = nextrow_func
        self.store_func = store_func

    def allocate_buf(self):
        return self.free_buffers.pop()

    def deallocate_buf(self, buffer):
        self.free_buffers.add(buffer)

}}

{{def reduce_worker(ctx, row_start, row_stop, out_vname, is_top, store_func)}}
  {{py: out_slice_vname = out_vname if is_top or out_vname is None else '(%s + WS)' % out_vname}}
  {{if row_stop - row_start == 1}}
    {{ctx.nextrow_func(row_start, out_slice_vname)}}
  {{else}}
    /* Pass on output buffer to children if is_top, else allocate a new
       one. */
    {{py: buf_vname = out_vname if is_top else ctx.allocate_buf()}}
    {{py: nrows = row_stop - row_start}}
    {{reduce_worker(ctx, row_start, row_start + idiv(nrows, 2), buf_vname, True, None)}}
    {{reduce_worker(ctx, row_start + idiv(nrows, 2), row_stop, buf_vname, False, None)}}

    /* Do pairwise reduction of values in buf and store result in out.
       For the final output, we call user code to do the store. */
    {
      double _reduced_val = {{buf_vname}}[reduce_idx] + {{buf_vname}}[reduce_idx + 2];
      {{if store_func is not None}}
        /* we are asked to store reduced value directly to target */
        int _reduced_j = ithread % 2;
        int _reduced_k = ((ithread % WS) / 2);
        {{store_func('_reduced_j', '_reduced_k', '_reduced_val')}}
      {{else}}
        /* inner level, store results for parent level */
        {{out_slice_vname}}[ithread % WS] = _reduced_val;
      {{endif}}
    }
    {{py: if not is_top: ctx.deallocate_buf(buf_vname)}}
  {{endif}}
{{enddef}}

{{def parallel_tree_reduction(*args)}}
/*
  If nwarps == 1, we pass the store_func to reduce_worker so that the final
  intra-warp reduction stores values. Otherwise, we proceed to inter-warp
  reduction and call store_func at the end of that.
*/
  {{py: ctx = ReductionContext(*args)}}
  {{py: root_buf = ctx.allocate_buf()}}
  {{py: body = reduce_worker(ctx, 0, 16, root_buf, True, ctx.store_func if nwarps == 1 else None)}}

  int pair_idx = (ithread % WS) / 2;
  int reduce_idx = 4 * pair_idx + ithread % 2;
  /* Intra-warp reduction */
  {{body}}
  {{if nwarps > 1}}
    /* Inter-warp reduction within root_buf. The following is not an
       optimal work-load distribution scheme, but we seem to be targeting
       nwarps==2 at the time of writing... */
    __syncthreads();
    if (iwarp == 0) {
      double _reduced_val = 0;
      for (int t = 0; t != NWARPS; ++t) {
        /* This assumes buffers are contigously allocated between warps.. */
        _reduced_val += ({{root_buf}} + t * 2 * WS)[ithread];
      }
      /* Call with expressions for (j, k, value) */
      {{ctx.store_func('(ithread % 2)', '(ithread / 2)', '_reduced_val')}}
    }
    __syncthreads();
  {{endif}}
{{enddef}}

{{py: assert nvecs % 2 == 0}}
{{if 'test_reduce_kernel' not in skip_kernels}}
__global__ void test_reduce_kernel(double *output, int repeat) {
  /* Generates array (nvecs, 16, nwarps) in output, consisting
     of sum_{over warp} ( ithread * irow * (ivec + 1)) ),
     using the same parallel reduction algorithm as we use in
     transpose_legendre_transform. */

  /* Buffers for intra-warp communication. They are segmented into one
     independent area for each warp of size 2 * WS; no cross-warp talk. */
  {{for buf in reduction_buffers}}
    __shared__ volatile double _{{buf}}[2 * NTHREADS];
    volatile double *{{buf}} = _{{buf}} + 2 * iwarp * WS;
  {{endfor}}


  /* Move to block */
  output += NTHREADS * blockIdx.x;

  __shared__ double buffer[NTHREADS];

  /* Zero output -- note that 2*16*nwarps == nthreads. */
  buffer[ithread] = 0;
  __syncthreads();

  /* Registers for loading row of input*/
  {{def nextrow(rowidx, outbuf)}}
    /* Load next row into registers -- in transpose_legendre_transform,
       this step is replaced by computation. The "computation order" must
       be arranged so that vec-number is reversed for every other thread... */
  {
    int odd = ithread % 2;
    int j_mine = odd;
    int j_other = 1 - odd;
    double mine = ithread * {{rowidx}} * (j_mine + 1);
    double to_other = ithread * {{rowidx}} * (j_other + 1);
    /* Pair-of-two sum-reduction. Result is stored
       interleaved: (j=0, j=1, j=0, j=1, ...) */
    {{outbuf}}[ithread % 32 + 1 - 2 * odd] = to_other; /* Send to neighbour thread */
    {{outbuf}}[ithread % 32] += mine; /* Accumulate with what neighbour stored */
  }
  {{enddef}}
  {{def store(vec_code, row_code, value_code)}}
  buffer[{{vec_code}} * 16 * NWARPS + {{row_code}} * NWARPS + iwarp] += {{value_code}};
  {{enddef}}

  for (int rep = 0; rep != repeat; ++rep) {
    {{parallel_tree_reduction(nextrow, store, reduction_buffers)}}
  }

  __syncthreads();
  output[ithread] = buffer[ithread];

}
{{endif}}

/*****
 * Transposed Legendre transform
 *****/

/*
  The following routine is responsible for dealing with a block of Lambda
  of size (K_CHUNK, I_CHUNK * NTHREADS). The i-coordinate (of the upper-left
  corner) is explicitly passed in, while the k-coordinate is only implicitly
  given by the contents of input arrays.
*/
{{for blocktype in ('inner', 'edge')}}
__device__ void transpose_legendre_transform_{{blocktype}}_block(
    int i_start,
    const double *Lambda_0,
    const double *Lambda_1,
    const double *x_squared,
    const ushort *i_stops,
    const double *alpha,
    const double *beta,
    const double *gamma,
    double *Lambda_p,
    double *Lambda_pp,
    double *out_accumulator,
    const double *q,
    int q_stride,
    int nrows,
    {{', '.join(['volatile double *%s' % buf for buf in reduction_buffers])}}
) {

  /*
    Code snippet to be instantiated to compute each row, including 32->16 reduction.
  */
  {{def nextrow(kt, outbuf)}}
  {
    double acc_0 = 0, acc_1 = 0;
    double Lambda_val = 0;
    {{for ri in range(i_chunk)}}
    {
      int kc = k_offset + {{kt}};

      {{def Lambda_recurrence}}
          Lambda_val =
            ((x_squared_{{ri}} + alpha[kc]) * beta[kc] * Lambda_p_{{ri}} +
             gamma[kc] * Lambda_pp_{{ri}});
      {{enddef}}
      
      {{def matvec}}
          {{for rj in range(nvecs)}}
          acc_{{rj}} += Lambda_val * q_{{ri}}_{{rj}};
          {{endfor}}
      {{enddef}}

      {{def swap}}
          Lambda_pp_{{ri}} = Lambda_p_{{ri}};
          Lambda_p_{{ri}} = Lambda_val;
      {{enddef}}

      {{if blocktype == 'inner'}}
        {{Lambda_recurrence()}}
        {{matvec()}}
        {{swap()}}
      {{else}}
        int i = i_start + ithread + {{nthreads * ri}};
        /* Need to account for i_stops and state of Lambda_p/pp,
           and read initialization values from input if necesarry.

           Note the goto statement. */
        if (i >= i_stops[kc]) {
          goto done{{kt}};
        }
        if (IS_UNASSIGNED(Lambda_p_{{ri}})) {
          /* Have not started this column at all yet */
          Lambda_val = Lambda_0[i];
        } else if (IS_UNASSIGNED(Lambda_pp_{{ri}})) {
          /* Lambda_p != 0, Lambda_pp == 0 => Started in the previous iteration */
          Lambda_val = Lambda_1[i];
        } else {
          {{Lambda_recurrence()}}
        }
        {{matvec()}}
        {{swap()}}
      {{endif}}
    }
    {{endfor}}

    {{if blocktype == 'edge'}}
    done{{kt}}:
    {{endif}}
        
    /* Do 32->16 reduction and store result in outbuf. Note that
       acc_t corresponds to j=|t-odd|. */
    {
      {{outbuf}}[ithread % 32 + 1 - 2 * odd] = acc_1;
      {{outbuf}}[ithread % 32] += acc_0;
    }
  }
  {{enddef}}

  /*
    Code snippet for storing reduction results. Only executed for iwarp==0.
    k_offset is defined in a loop in the body below.
  */
  {{def store(vec_code, row_code, value_code)}}
  out_accumulator[k_offset * NVECS + ithread] += {{value_code}};
  {{enddef}}

  /*
    Body
   */

  /* Read Lambdas, x_squared, and q from global memory into
     registers for this block.  Shuffle q's so that for odd
     ithread, j=1 corresponds to rj=0; i.e., the odd threads
     compute j=1 before j=0. This is convenient when doing the
     initial 32->16 column reduction below.
  */
  int odd = ithread % 2;
  {{for ri in range(i_chunk)}}
    double Lambda_pp_{{ri}}, Lambda_p_{{ri}};
    double x_squared_{{ri}};
    double {{', '.join(['q_%d_%d' % (ri, rj) for rj in range(nvecs)])}};
    {
      int i = i_start + ithread + {{nthreads * ri}};
      Lambda_pp_{{ri}} = Lambda_pp[i_start / NTHREADS + {{ri}}];
      Lambda_p_{{ri}} = Lambda_p[i_start / NTHREADS + {{ri}}];
      x_squared_{{ri}} = x_squared[i];
      {{for rj in range(nvecs)}}
      q_{{ri}}_{{rj}} = q[({{'odd' if rj == 0 else '1 - odd'}}) * q_stride + i]  ;
      {{endfor}}
    }
  {{endfor}}

  /* Process all rows, using generation-and-reduction code.
     See above for definition of nextrow and store; essentially,
     `nextrow` computes for next k, while `store` accumulates in
     `out_buffer` of shape (NVECS, K_CHUNK) .

     We always work in chunks of 16 rows (as that is what the
     reduction code support), but then avoid copying from buffer
     to out beyond nk at the end of the block row.
  */
  for (int k_offset = 0; k_offset < nrows; k_offset += 16) {
    {{parallel_tree_reduction(nextrow, store, reduction_buffers)}}
  }

  /* Persist Lambdas for the benefit of the block below. */
  {{for ri in range(i_chunk)}}
  Lambda_pp[i_start / NTHREADS + {{ri}}] = Lambda_pp_{{ri}};
  Lambda_p[i_start / NTHREADS + {{ri}}] = Lambda_p_{{ri}};
  {{endfor}}

  
}
{{endfor}}{{#blocktype}}

/* The worker for performing a single transposed Legendre transform.
   All arguments are expected to be block-specific, i.e., the block
   index is never taken into account beyond this point.
*/
__device__ void transpose_legendre_transform_single(int m, int lmin, int nk, int ni,
                                                    const double *x_squared,
                                                    const double *Lambda_0,
                                                    const double *Lambda_1,
                                                    const ushort *i_stops,
                                                    const double *q,
                                                    double *a,
                                                    int zero) {
  
  /* Scratch for auxiliary values */
  __shared__ double alpha[K_CHUNK], beta[K_CHUNK], gamma[K_CHUNK];
  __shared__ ushort i_stops_chunk[K_CHUNK];
  __shared__ double out_buffer[NVECS * K_CHUNK];

  {{for buf in reduction_buffers}}
    __shared__ volatile double _{{buf}}[2 * NTHREADS];
    volatile double *{{buf}} = _{{buf}} + 2 * iwarp * WS;
  {{endfor}}


  for (int idx = ithread; idx < NVECS * K_CHUNK; idx += NTHREADS) {
    out_buffer[idx] = 0;
  }

  /* Buffer for temporarily storing Legendre function values,
     stored in global/CUDA-local memory. Access to these arrays is
     amortized over K_CHUNK. */
  double Lambda_pp[MAX_NCHUNKS_I * I_CHUNK];
  double Lambda_p[MAX_NCHUNKS_I * I_CHUNK];

  for (int ri = 0; ri < I_CHUNK * MAX_NCHUNKS_I; ++ri) {
    Lambda_pp[ri] = UNASSIGNED;
    Lambda_p[ri] = UNASSIGNED;
  }
  /* No barrier: Will hit barrier below before work_local_sum gets
     overwritten again, and we only write once to each element in
     out. */


  /* Blocking approach: Process blocks of size (K_CHUNK, I_CHUNK * NTHREADS).
     Within each block, local memory contains data needed for rows (k-axis),
     and registers contains data needed for columns (i-axis).
     The blocks are processed in row-wise order.
  */
  for (int k_block_start = 0; k_block_start < nk; k_block_start += K_CHUNK) {
    int k_block_len = min(K_CHUNK, nk - k_block_start);
    /* Put values invariant over row-block in shared memory:
         - Compute auxiliaries
         - Copy i_stops 
    */
    compute_recurrence_auxiliaries(m, lmin + 2 * k_block_start,
                                   alpha, beta, gamma);
    {
      int idx;
      for (idx = ithread; idx < k_block_len; idx += NTHREADS) {
        i_stops_chunk[idx] = i_stops[k_block_start + idx];
      }
      for (; idx < K_CHUNK; idx += NTHREADS) {
        i_stops_chunk[idx] = 0;
      }
    }
    __syncthreads();
    
    /* Process all blocks on this block-row. First do all inner blocks
       where one does not need to check for i_stop or whether Lambda's
       are initialize; then do the edge blocks.
     */


    int i_block_start = 0;
    {{for blocktype in ('inner', 'edge',)}}
    {
      int i_stop;
      {{if blocktype == 'inner'}}
        if (k_block_start == 0 || i_stops_chunk[0] < I_CHUNK * NTHREADS) {
          /* For top edge, skip inner block loop. And don't allow blocks
             that would be too narrow. */
          i_stop = 0; 
        } else {
          /* Do not allow inner blocks to touch edge. We subtract the blockwidth
             here -- note that `i_stop` is the stop of the *left* edge of the block
             in the for-loop.
           */
          i_stop = i_stops_chunk[0] - I_CHUNK * NTHREADS;
        }
      {{else}}
        i_stop = i_stops_chunk[k_block_len - 1];
      {{endif}}
      for (; i_block_start < i_stop; i_block_start += I_CHUNK * NTHREADS) {
        transpose_legendre_transform_{{blocktype}}_block(
            i_block_start,
            Lambda_0,
            Lambda_1,
            x_squared,
            i_stops_chunk,
            alpha,
            beta,
            gamma,
            Lambda_p,
            Lambda_pp,
            out_buffer,
            q,
            ni /* q_stride */,
            k_block_len,  /* nrows */
            {{','.join([buf for buf in reduction_buffers])}}
         );
      }
    }
    {{endfor}}
    /* Done with row block. Copy results from out_buffer to out, AND
       zero out_buffer. Note: Each iteration of parallel_tree_reduction
       ends with __syncthreads(), so threads are synced at this
       point. */
    {{check(nvecs == nwarps == 2, 'not (nvecs == nwarps == 2)')}}
    {
      /* Values in out_buffer are stored in strided order [j=0, j=1, j=1, ...]. */
      int j = ithread % NVECS;
      for (int k_offset = 0; k_offset != K_CHUNK; k_offset += NTHREADS / NVECS) {
        int kt = ithread / NVECS;
        int k = k_block_start + k_offset + kt;
        int idx = (k_offset + kt) * NVECS + j;
        if (k < nk) {
          a[2 * k * NVECS + j] = out_buffer[idx];
        }
        out_buffer[idx] = 0;
      }
    }
    /* Do not need another barrier here because the next iteration will
       hit the barrier after computing auxiliary values before it
       gets a chance to overwrite any accumulation buffers. */
  }
}


/*
  Kernel for performing many similar Legendre transforms for the
  same m. Used for benchmarking and debugging.

  The same `x_squared` is assumed for every block.

  `a` is the output, of shape (2 * nk, NVECS). Only every other
  row of `a` is written to (corresponding to the even/odd elements)
*/
{{if 'transpose_legendre_transform' not in skip_kernels}}
__global__ void transpose_legendre_transform(int m, int lmin, int nk, int ni,
                                             const double *x_squared,
                                             const double *Lambda_0,
                                             const double *Lambda_1,
                                             const ushort *i_stops,
                                             const double *q,
                                             double *a,
                                             int zero) {
  /* Shift to our block. */
  Lambda_0 += blockIdx.x * ni;
  Lambda_1 += blockIdx.x * ni;
  i_stops += blockIdx.x * nk;
  q += blockIdx.x * ni * NVECS;
  a += blockIdx.x * (2 * nk) * NVECS;
  /* Call worker */
  transpose_legendre_transform_single(m, lmin, nk, ni, x_squared, Lambda_0,
                                      Lambda_1, i_stops, q, a, zero);
}
{{endif}}


/*
Perform all the Legendre transforms necesarry for a SHT. We assume a block
grid of size (lmax + 1, 2) and use one block per Legendre transform.

Arguments
---------
  lmax, ni
  resources: buffer of resource data, which is expected to be laid out as:

   - double[ni] x_squared
   - For each m = 0..lmax:
        For each odd:
            double[ni] Lambda_0
            double[ni] Lambda_1
            ushort[ni] i_stops

  q: Input data of shape (lmax + 1, 2, NVECS, ni) indexed by [m, odd, j, i]
  a: Output data of shape ((lmax + 1)**2, NVECS), stored contiguously in
     m-major ordering
*/

{{if "all_transpose_legendre_transforms" not in skip_kernels}}


/* Gets from (l,m) to array index in mmajor ordering */
inline __device__ uint lm_to_idx_mmajor(uint l, uint m, uint lmax) {
  return m * (2 * lmax - m + 3) / 2 + (l - m);
}

inline __device__ uint get_nk(uint lmax, uint m, uint odd) {
  return (lmax + 1 - m - odd + 1) / 2;
}

__global__ void all_transpose_legendre_transforms(uint lmax,
                                                  uint mmin,
                                                  uint ni,
                                                  const char *resources,
                                                  const double *q,
                                                  double *a,
                                                  int zero) {
  uint m = mmin + blockIdx.x;
  uint odd = blockIdx.y;
  uint idx = lm_to_idx_mmajor(m + odd, m, lmax);
  uint lmin = m + odd;

  /* Find resource tables */
  const char *head = resources;
  head += 2 * (lmax + 1) * 8; /* Skip nnz */

  const double *x_squared = (double*)head; head += sizeof(double) * ni;
  const double *Lambda_0 = (double*)head; head += sizeof(double) * ni * 2 * (lmax + 1);
  const double *Lambda_1 = (double*)head; head += sizeof(double) * ni * 2 * (lmax + 1);
  const ushort *i_stops = (ushort*)head;

  /* Shift to our block's data */
  q += (2 * m + odd) * (NVECS * ni);
  a += lm_to_idx_mmajor(lmin, m, lmax) * NVECS;

  Lambda_0 += (2 * m + odd) * ni;
  Lambda_1 += (2 * m + odd) * ni;
  i_stops += lm_to_idx_mmajor(m, m, lmax);
  /* Now i_stops points to beginning of even coefficients */
  if (odd) i_stops += get_nk(lmax, m, 0);
  
  /* Let's go! */
  uint nk = get_nk(lmax, m, odd);
  transpose_legendre_transform_single(m, lmin, nk, ni, x_squared, Lambda_0,
                                      Lambda_1, i_stops, q, a, zero);
  
}
{{endif}}
